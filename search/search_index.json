{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#xgbse-xgboost-survival-embeddings","title":"<code>xgbse</code>: XGBoost Survival Embeddings","text":"<p>\"There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.\"- Leo Breiman, Statistical Modeling: The Two Cultures</p> <p>Survival Analysis is a powerful statistical technique with a wide range of applications such as predictive maintenance, customer churn, credit risk, asset liquidity risk, and others.</p> <p>However, it has not yet seen widespread adoption in industry, with most implementations embracing one of two cultures:</p> <ol> <li>models with sound statistical properties, but lacking in expressivess and computational efficiency</li> <li>highly efficient and expressive models, but lacking in statistical rigor</li> </ol> <p><code>xgbse</code> aims to unite the two cultures in a single package, adding a layer of statistical rigor to the highly expressive and computationally effcient <code>xgboost</code> survival analysis implementation.</p> <p>The package offers:</p> <ul> <li>calibrated and unbiased survival curves with confidence intervals (instead of point predictions)</li> <li>great predictive power, competitive to vanilla <code>xgboost</code></li> <li>efficient, easy to use implementation</li> <li>explainability through prototypes</li> </ul> <p>This is a research project by Loft Data Science Team, however we invite the community to contribute. Please help by trying it out, reporting bugs, and letting us know what you think!</p>"},{"location":"basic-usage.html","title":"Basic Usage","text":""},{"location":"basic-usage.html#api","title":"API","text":"<p>The package follows <code>scikit-learn</code> API, with a minor adaptation to work with time and event data (<code>y</code> as a <code>numpy</code> structured array of times and events). <code>.predict()</code> returns a dataframe where each column is a time window and values represent the probability of survival before or exactly at the time window.</p> <pre><code># importing dataset from pycox package\nfrom pycox.datasets import metabric\n\n# importing model and utils from xgbse\nfrom xgbse import XGBSEKaplanNeighbors\nfrom xgbse.converters import convert_to_structured\n\n# getting data\ndf = metabric.read_df()\n\n# splitting to X, y format\nX = df.drop(['duration', 'event'], axis=1)\ny = convert_to_structured(df['duration'], df['event'])\n\n# fitting xgbse model\nxgbse_model = XGBSEKaplanNeighbors(n_neighbors=50)\nxgbse_model.fit(X, y)\n\n# predicting\nevent_probs = xgbse_model.predict(X)\nevent_probs.head()</code></pre> index 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 0 0.98 0.87 0.81 0.74 0.71 0.66 0.53 0.47 0.42 0.4 0.3 0.25 0.21 0.16 0.12 0.098 0.085 0.062 0.054 1 0.99 0.89 0.79 0.7 0.64 0.58 0.53 0.46 0.42 0.39 0.33 0.31 0.3 0.24 0.21 0.18 0.16 0.11 0.095 2 0.94 0.78 0.63 0.57 0.54 0.49 0.44 0.37 0.34 0.32 0.26 0.23 0.21 0.16 0.13 0.11 0.098 0.072 0.062 3 0.99 0.95 0.93 0.88 0.84 0.81 0.73 0.67 0.57 0.52 0.45 0.37 0.33 0.28 0.23 0.19 0.16 0.12 0.1 4 0.98 0.92 0.82 0.77 0.72 0.68 0.63 0.6 0.57 0.55 0.51 0.48 0.45 0.42 0.38 0.33 0.3 0.22 0.2 <p>You can also get interval predictions (probability of failing exactly at each time window) using <code>return_interval_probs</code>:</p> <pre><code># point predictions\ninterval_probs = xgbse_model.predict(X_valid, return_interval_probs=True)\ninterval_probs.head()</code></pre> index 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 0 0.024 0.1 0.058 0.07 0.034 0.05 0.12 0.061 0.049 0.026 0.096 0.049 0.039 0.056 0.04 0.02 0.013 0.023 0.0078 1 0.014 0.097 0.098 0.093 0.052 0.065 0.054 0.068 0.034 0.038 0.053 0.019 0.018 0.052 0.038 0.027 0.018 0.05 0.015 2 0.06 0.16 0.15 0.054 0.033 0.053 0.046 0.073 0.032 0.014 0.06 0.03 0.017 0.055 0.031 0.016 0.014 0.027 0.0097 3 0.011 0.034 0.021 0.053 0.038 0.038 0.08 0.052 0.1 0.049 0.075 0.079 0.037 0.052 0.053 0.041 0.026 0.04 0.017 4 0.016 0.067 0.099 0.046 0.05 0.042 0.051 0.028 0.03 0.018 0.048 0.022 0.029 0.038 0.035 0.047 0.031 0.08 0.027"},{"location":"basic-usage.html#survival-curves-and-confidence-intervals","title":"Survival curves and confidence intervals","text":"<p><code>XBGSEKaplanTree</code> and <code>XBGSEKaplanNeighbors</code> support estimation of survival curves and confidence intervals via the Exponential Greenwood formula out-of-the-box via the <code>return_ci</code> argument:</p> <pre><code># fitting xgbse model\nxgbse_model = XGBSEKaplanNeighbors(n_neighbors=50)\nxgbse_model.fit(X_train, y_train, time_bins=TIME_BINS)\n\n# predicting\nmean, upper_ci, lower_ci = xgbse_model.predict(X_valid, return_ci=True)\n\n# plotting CIs\nplot_ci(mean, upper_ci, lower_ci)</code></pre> <p></p> <p><code>XGBSEDebiasedBCE</code> does not support estimation of confidence intervals out-of-the-box, but we provide the <code>XGBSEBootstrapEstimator</code> to get non-parametric confidence intervals. As the stacked logistic regressions are trained with more samples (in comparison to neighbor-sets in <code>XGBSEKaplanNeighbors</code>), confidence intervals are more concentrated:</p> <pre><code># base model as BCE\nbase_model = XGBSEDebiasedBCE(PARAMS_XGB_AFT, PARAMS_LR)\n\n# bootstrap meta estimator\nbootstrap_estimator = XGBSEBootstrapEstimator(base_model, n_estimators=20)\n\n# fitting the meta estimator\nbootstrap_estimator.fit(\n    X_train,\n    y_train,\n    validation_data=(X_valid, y_valid),\n    early_stopping_rounds=10,\n    time_bins=TIME_BINS,\n)\n\n# predicting\nmean, upper_ci, lower_ci = bootstrap_estimator.predict(X_valid, return_ci=True)\n\n# plotting CIs\nplot_ci(mean, upper_ci, lower_ci)</code></pre> <p></p> <p>The bootstrap abstraction can be used for <code>XBGSEKaplanTree</code> and <code>XBGSEKaplanNeighbors</code> as well, however, the confidence interval will be estimated via bootstrap only (not Exponential Greenwood formula):</p> <pre><code># base model\nbase_model = XGBSEKaplanTree(PARAMS_TREE)\n\n# bootstrap meta estimator\nbootstrap_estimator = XGBSEBootstrapEstimator(base_model, n_estimators=100)\n\n# fitting the meta estimator\nbootstrap_estimator.fit(\n    X_train,\n    y_train,\n    time_bins=TIME_BINS,\n)\n\n# predicting\nmean, upper_ci, lower_ci = bootstrap_estimator.predict(X_valid, return_ci=True)\n\n# plotting CIs\nplot_ci(mean, upper_ci, lower_ci)</code></pre> <p></p> <p>With a sufficiently large <code>n_estimators</code>, interval width shouldn't be much different, with the added benefit of model stability and improved accuracy. Addittionaly, <code>XGBSEBootstrapEstimator</code> allows building confidence intervals for interval probabilities (which is not supported for Exponential Greenwood):</p> <pre><code># predicting\nmean, upper_ci, lower_ci = bootstrap_estimator.predict(\n    X_valid,\n    return_ci=True,\n    return_interval_probs=True\n)\n\n# plotting CIs\nplot_ci(mean, upper_ci, lower_ci)</code></pre> <p></p> <p>The parameter <code>ci_width</code> controls the width of the confidence interval. For <code>XGBSEKaplanTree</code> it should be passed at <code>.fit()</code>, as KM curves are pre-calculated for each leaf at fit time to avoid storing training data.</p> <pre><code># fitting xgbse model\nxgbse_model = XGBSEKaplanTree(PARAMS_TREE)\nxgbse_model.fit(X_train, y_train, time_bins=TIME_BINS, ci_width=0.99)\n\n# predicting\nmean, upper_ci, lower_ci = xgbse_model.predict(X_valid, return_ci=True)\n\n# plotting CIs\nplot_ci(mean, upper_ci, lower_ci)</code></pre> <p></p> <p>For other models (<code>XGBSEKaplanNeighbors</code> and <code>XGBSEBootstrapEstimator</code>) it should be passed at <code>.predict()</code>.</p> <pre><code># base model\nmodel = XGBSEKaplanNeighbors(PARAMS_XGB_AFT, N_NEIGHBORS)\n\n# fitting the meta estimator\nmodel.fit(\n    X_train, y_train,\n    validation_data = (X_valid, y_valid),\n    early_stopping_rounds=10,\n    time_bins=TIME_BINS\n)\n\n# predicting\nmean, upper_ci, lower_ci = model.predict(X_valid, return_ci=True, ci_width=0.99)\n\n# plotting CIs\nplot_ci(mean, upper_ci, lower_ci)</code></pre> <p></p>"},{"location":"basic-usage.html#early-stopping","title":"Early stopping","text":"<p>A simple interface to <code>xgboost</code> early stopping is provided.</p> <pre><code># splitting between train, and validation\n(X_train, X_valid,\n y_train, y_valid) = \\\ntrain_test_split(X, y, test_size=0.2, random_state=42)\n\n# fitting with early stopping\nxgb_model = XGBSEDebiasedBCE()\nxgb_model.fit(\n    X_train,\n    y_train,\n    validation_data=(X_valid, y_valid),\n    early_stopping_rounds=10,\n    verbose_eval=50\n)</code></pre> <pre><code>[0] validation-aft-nloglik:16.86713\nWill train until validation-aft-nloglik hasn't improved in 10 rounds.\n[50]    validation-aft-nloglik:3.64540\n[100]   validation-aft-nloglik:3.53679\n[150]   validation-aft-nloglik:3.53207\nStopping. Best iteration:\n[174]   validation-aft-nloglik:3.53004</code></pre>"},{"location":"basic-usage.html#explainability-through-prototypes","title":"Explainability through prototypes","text":"<p><code>xgbse</code> also provides explainability through prototypes, searching the embedding for neighbors. The idea is to explain model predictions with real samples, providing solid ground to justify them (see [8]). The method <code>.get_neighbors()</code> searches for the <code>n_neighbors</code> nearest neighbors in <code>index_data</code> for each sample in <code>query_data</code>:</p> <pre><code>neighbors = xgb_model.get_neighbors(\n    query_data=X_valid,\n    index_data=X_train,\n    n_neighbors=10\n)\nneighbors.head(5)</code></pre> index neighbor_1 neighbor_2 neighbor_3 neighbor_4 neighbor_5 neighbor_6 neighbor_7 neighbor_8 neighbor_9 neighbor_10 1225 1151 1513 1200 146 215 452 1284 1127 1895 257 111 1897 1090 1743 1224 892 1695 1624 1546 1418 4 554 9 627 1257 1460 1031 1575 1557 440 1236 858 526 726 1042 177 1640 242 1529 234 1800 399 1431 1313 205 1738 599 954 1694 1715 1651 828 541 992 <p>This way, we can inspect neighbors of a given sample to try to explain predictions. For instance, we can choose a <code>reference</code> and check that its neighbors actually are very similar as a sanity check:</p> <pre><code>i = 0\n\nreference = X_valid.iloc[i]\nreference.name = 'reference'\ntrain_neighs = X_train.loc[neighbors.iloc[i]]\n\npd.concat([reference.to_frame().T, train_neighs])</code></pre> index x0 x1 x2 x3 x4 x5 x6 x7 x8 reference 5.7 5.7 11 5.6 1 1 0 1 86 1151 5.8 5.9 11 5.5 1 1 0 1 82 1513 5.5 5.5 11 5.6 1 1 0 1 79 1200 5.7 6 11 5.6 1 1 0 1 76 146 5.9 5.9 11 5.5 0 1 0 1 75 215 5.8 5.5 11 5.4 1 1 0 1 78 452 5.7 5.7 12 5.5 0 0 0 1 76 1284 5.6 6.2 11 5.6 1 0 0 1 79 1127 5.5 5.1 11 5.5 1 1 0 1 86 1895 5.5 5.4 10 5.5 1 1 0 1 85 257 5.7 6 9.6 5.6 1 1 0 1 76 <p>We also can compare the Kaplan-Meier curve estimated from the neighbors to the actual model prediction, checking that it is inside the confidence interval:</p> <pre><code>from xgbse.non_parametric import calculate_kaplan_vectorized\n\nmean, high, low = calculate_kaplan_vectorized(\n    np.array([y['c2'][neighbors.iloc[i]]]),\n    np.array([y['c1'][neighbors.iloc[i]]]),\n    TIME_BINS\n)\n\nmodel_surv = xgb_model.predict(X_valid)\n\nplt.figure(figsize=(12,4), dpi=120)\nplt.plot(model_surv.columns, model_surv.iloc[i])\nplt.plot(mean.columns, mean.iloc[0])\nplt.fill_between(mean.columns, low.iloc[0], high.iloc[0], alpha=0.1, color='red')</code></pre> <p></p> <p>Specifically, for <code>XBGSEKaplanNeighbors</code> prototype predictions and model predictions should match exactly if <code>n_neighbors</code> is the same and <code>query_data</code> is equal to the training data.</p>"},{"location":"basic-usage.html#metrics","title":"Metrics","text":"<p>We made our own metrics submodule to make the lib self-contained. <code>xgbse.metrics</code> implements C-index, Brier Score and D-Calibration from [9], including adaptations to deal with censoring:</p> <pre><code># training model\nxgbse_model = XGBSEKaplanNeighbors(PARAMS_XGB_AFT, n_neighbors=30)\n\nxgbse_model.fit(\n    X_train, y_train,\n    validation_data = (X_valid, y_valid),\n    early_stopping_rounds=10,\n    time_bins=TIME_BINS\n)\n\n# predicting\npreds = xgbse_model.predict(X_valid)\n\n# importing metrics\nfrom xgbse.metrics import (\n    concordance_index,\n    approx_brier_score,\n    dist_calibration_score\n)\n\n# running metrics\nprint(f'C-index: {concordance_index(y_valid, preds)}')\nprint(f'Avg. Brier Score: {approx_brier_score(y_valid, preds)}')\nprint(f\"\"\"D-Calibration: {dist_calibration_score(y_valid, preds) &gt; 0.05}\"\"\")</code></pre> <p><pre><code>C-index: 0.6495863029409356\nAvg. Brier Score: 0.1704190044350422\nD-Calibration: True</code></pre> As metrics follow the <code>score_func(y, y_pred, **kwargs)</code> pattern, we can use the sklearn model selection module easily:</p> <p><pre><code>from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nxgbse_model = XGBSEKaplanTree(PARAMS_TREE)\nresults = cross_val_score(xgbse_model, X, y, scoring=make_scorer(approx_brier_score))\nresults</code></pre> <pre><code>array([0.17432953, 0.15907712, 0.13783666, 0.16770409, 0.16792016])</code></pre></p>"},{"location":"basic-usage.html#citing-xgbse","title":"Citing <code>xgbse</code>","text":"<p>To cite this repository:</p> <pre><code>@software{xgbse2020github,\n  author = {Davi Vieira and Gabriel Gimenez and Guilherme Marmerola and Vitor Estima},\n  title = {XGBoost Survival Embeddings: improving statistical properties of XGBoost survival analysis implementation},\n  url = {http://github.com/loft-br/xgboost-survival-embeddings},\n  version = {0.2.0},\n  year = {2020},\n}</code></pre>"},{"location":"basic-usage.html#references","title":"References","text":"<p>[1] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, and J. Q. Candela. Practical Lessons from Predicting Clicks on Ads at Facebook (2014). In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising (ADKDD\u201914).</p> <p>[2] Feature transformations with ensembles of trees. Scikit-learn documentation at https://scikit-learn.org/.</p> <p>[3] G. Marmerola. Calibration of probabilities for tree-based models. Personal Blog at https://gdmarmerola.github.io/.</p> <p>[4] G. Marmerola. Supervised dimensionality reduction and clustering at scale with RFs with UMAP. Personal Blog at https://gdmarmerola.github.io/.</p> <p>[5] C. Yu, R. Greiner, H. Lin, V. Baracos. Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors. Advances in Neural Information Processing Systems 24 (NIPS 2011).</p> <p>[6] H. Kvamme, \u00d8. Borgan. The Brier Score under Administrative Censoring: Problems and Solutions. arXiv preprint arXiv:1912.08581.</p> <p>[7] S. Sawyer. The Greenwood and Exponential Greenwood Confidence Intervals in Survival Analysis. Handout on Washington University in St. Louis website.</p> <p>[8] H. Kvamme, \u00d8. Borgan, and I. Scheel. Time-to-event prediction with neural networks and Cox regression. Journal of Machine Learning Research, 20(129):1\u201330, 2019.</p> <p>[9] H. Haider, B. Hoehn, S. Davis, R. Greiner. Effective Ways to Build and Evaluate Individual Survival Distributions. Journal of Machine Learning Research 21 (2020) 1\u201363.</p>"},{"location":"how_xgbse_works.html","title":"How XGBSE works","text":"<p>In this section, we try to make a quick introduction to <code>xgbse</code>. Refer to this this Notebook for the full code and/or if you want a more practical introduction.</p>"},{"location":"how_xgbse_works.html#what-xgbse-tries-to-solve","title":"What <code>xgbse</code> tries to solve","text":"<p>The XGBoost implementation provides two methods for survival analysis: Cox and Accelerated Failure Time (AFT). When it comes to ordering individuals by risk, both show competitive performance (as measured by C-index, the ROC AUC equivalent for survival) while being lightning fast.</p> <p>However, we can observe shortcomings when it comes to other desirable statistical properties. Specifically, three properties are of concern:</p> <ul> <li>prediction of survival curves rather than point estimates</li> <li>estimation of confidence intervals</li> <li>calibrated (unbiased) expected survival times</li> </ul> <p>Let us take the AFT implementation as an example (as Cox only outputs risk and not time or survival). The AFT model outputs a value that should be interpreted as the expected survival time for each sample. To compute this time, it assumes an underlying distribution for times and events, controlled by the <code>aft_loss_distribution</code> and <code>aft_loss_distribution_scale</code> hyperparameters. As they control hard-wired assumptions of the model, we would expect that they could drastically change its output.</p> <p> <sub>Table explaining the impact of <code>aft_loss_distribution</code> and <code>aft_loss_distribution_scale</code>, the latter represented by the \\(z\\) variable in the formulas. Source: XGBoost AFT documentation.</sub> </p> <p>To confirm this is true, let us perform an experiment using the METABRIC dataset. We will test different values for <code>aft_loss_distribution_scale</code> while keeping <code>aft_loss_distribution</code> as <code>\"normal\"</code> and check how this will affect model performance. We test the following values: <code>[0.5, 1.0, 1.5]</code>. The following snippet gives a rough idea on how to run this simple experiment but you can find the full code here. The results are very interesting, if not alarming:</p> <pre><code>import xgboost as xgb\nfrom xgbse.metrics import concordance_index\nfrom xgbse._kaplan_neighbors import DEFAULT_PARAMS\n\n# dummy function representing dataprep\ndtrain, dval = prepare_data()\n\n# loop to show different scale results\nfor scale in [1.5, 1.0, 0.5]:\n\n    # chaning parameter\n    DEFAULT_PARAMS['aft_loss_distribution_scale'] = scale\n\n    # training model\n    bst = xgb.train(\n        DEFAULT_PARAMS,\n        dtrain,\n        num_boost_round=1000,\n        early_stopping_rounds=10,\n        evals=[(dval, 'val')],\n        verbose_eval=0\n    )\n\n    # predicting and evaluating\n    preds = bst.predict(dval)\n    cind = concordance_index(y_valid, -preds, risk_strategy=\"precomputed\")\n\n    print(f\"aft_loss_distribution_scale: {scale}\")\n    print(f\"C-index: {cind:.3f}\")\n    print(f\"Average survival time: {preds.mean():.0f} days\")\n    print(\"----\")</code></pre> <pre><code>aft_loss_distribution_scale: 1.5\nC-index: 0.645\nAverage survival time: 203 days\n----\naft_loss_distribution_scale: 1.0\nC-index: 0.648\nAverage survival time: 165 days\n----\naft_loss_distribution_scale: 0.5\nC-index: 0.646\nAverage survival time: 125 days\n----</code></pre> <p>In all three scenarios, we can build models with C-index results close to the state of the art [8]. However, our predictions change drastically, with two models showing an average predicted survival time difference of 78 days. This difference would perhaps be understandable if we were analyzing a single sample. But we're looking across the full validation dataset where the average prediction should be fairly stable regardless of which model we use.</p> <p>If we plot these results alongside an unbiased survival estimator such as the Kaplan Meier we can check that for each step of <code>0.5</code> in <code>aft_loss_distribution_scale</code> we move roughly one decile to the right in the curve. Also, we don't see a full survival curve: XGBoost only outputs point time-to-event predictions (no confidence intervals either).</p> <p> <sub>Vanilla XGBoost outputs predictions that are overly sensitive to hyperparameters, which prevents its use on applications that are sensitive to survival curve calibration.</sub> </p> <p>So what predictions should we trust? Such sensitivity to hyperparameters (<code>0.003</code> C-index variation yet 78 days difference) raises red flags for applications that are dependent on robust and calibrated time-to-event estimates, mining trust and preventing shipping survival analysis models to production.</p>"},{"location":"how_xgbse_works.html#leveraging-xgboost-as-a-feature-transformer","title":"Leveraging <code>xgboost</code> as a feature transformer","text":"<p>Although in need of an extension for statistical rigor, <code>xgboost</code> is still a powerhouse. C-index results show that the model can capture a great deal of signal, being competitive with the state of the art. We just need to adapt how we use it.</p> <p>Besides being leveraged for prediction tasks, Gradient Boosted Trees (GBTs) can also be used as feature transformers of the input data. Trees in the ensemble perform splits on features that discriminate the target, encoding the most relevant information for the task at hand in their structure. In particular, the terminal nodes (leaves) at each tree in the ensemble define a feature transformation (embedding) of the input data.</p> <p> <sub>We can extract features from a forest model like XGBoost, transforming the original feature space into a \"leaf occurrence\" embedding. Orange nodes represent the path of a single sample in the ensemble. </sub> </p> <p>This kind of tree ensemble embedding has very convenient properties:</p> <ol> <li> <p>sparsity and high-dimensionality: trees deal with nonlinearity and cast original features to a sparse, high-dimensional embedding, which helps linear models perform well when trained on it. This allows a Logistic Regression trained on the embedding (as one-hot encoded leaf indices) to have comparable performance to the actual ensemble, with the added benefit of probability calibration (see [1], [2], and [3])</p> </li> <li> <p>supervision: trees also work as a noise filter, performing splits only through features that have predictive power. Thus, the embedding actually has a lower intrinsic dimension than the input data. This mitigates the curse of dimensionality and allows a K-Nearest Neighbor model trained on the embedding (using hamming distance) to have comparable performance to the actual ensemble, with the added flexibility to apply any function over the neighbor-sets to get predictions. This arbitrary function can be, for instance, an unbiased survival estimator such as the Kaplan-Meier estimator (see [4])</p> </li> </ol> <p>We take advantage of these properties in different ways as we will show in the next subsections.</p>"},{"location":"how_xgbse_works.html#xgbsedebiasedbce-logistic-regressions-time-windows-embedding-as-input","title":"<code>XGBSEDebiasedBCE</code>: logistic regressions, time windows, embedding as input","text":"<p>Our first approach, <code>XGBSEDebiasedBCE</code>, takes inspiration from the multi-task logistic regression method in [5], the BCE approach in [6], and the probability calibration ideas from [1], [2] and [3].</p> <p>It consists of training a set of logistic regressions on top of the embedding produced by <code>xgboost</code>, each predicting survival at different user-defined discrete time windows. The classifiers remove individuals as they are censored, with targets that are indicators of surviving at each window.</p> <p></p> <p>The naive approach tends to give biased survival curves, due to the removal of censored individuals. Thus, we made some adaptations such that logistic regressions estimate the <code>di/ni</code> term (point probabilities) in the Kaplan-Meier formula and then use the KM estimator to get nearly unbiased survival curves.</p> <p>This way, we can get full survival curves from <code>xgboost</code>, and confidence intervals with minor adaptations (such as performing some rounds of bootstrap).</p> <p>Training and scoring of logistic regression models is efficient, being performed in parallel through <code>joblib</code>, so the model can scale to hundreds of thousands or millions of samples.</p>"},{"location":"how_xgbse_works.html#xgbsestackedweibull-xgboost-as-risk-estimator-weibull-aft-for-survival-curve","title":"<code>XGBSEStackedWeibull</code>: XGBoost as risk estimator, Weibull AFT for survival curve","text":"<p>In <code>XGBSEStackedWeibull</code>, we perform stacking of a XGBoost survival model with a Weibull AFT parametric model. The XGBoost fits the data and then predicts a value that is interpreted as a risk metric. This risk metric is fed to the Weibull regression which uses it as its only independent variable.</p> <p>Thus, we can get the benefit of XGBoost discrimination power alongside the Weibull AFT statistical rigor (e.g. calibrated survival curves).</p> <p></p> <p>As we're stacking XGBoost with a single, one-variable parametric model (as opposed to <code>XGBSEDebiasedBCE</code>), the model can be much faster (especially in training). We also have better extrapolation capabilities, due to stronger assumptions about the shape of the survival curve.</p> <p>However, these stronger assumptions may not fit some datasets as well as other methods.</p>"},{"location":"how_xgbse_works.html#xgbsekaplanneighbors-kaplan-meier-on-nearest-neighbors","title":"<code>XGBSEKaplanNeighbors</code>: Kaplan-Meier on nearest neighbors","text":"<p>As explained in the previous section, even though the embedding produced by <code>xgboost</code> is sparse and high dimensional, its intrisic dimensionality actually should be lower than the input data. This enables us to \"convert\" <code>xgboost</code> into a nearest neighbor model, where we use hamming distance to define similar elements as the ones that co-occurred the most at the ensemble terminal nodes. Then, at each neighbor-set we can get survival estimates with robust methods such as the Kaplan-Meier estimator.</p> <p></p> <p>We recommend using <code>dart</code> as the booster to prevent any tree to dominate variance in the ensemble and break the leaf co-ocurrence similarity logic. We built a high-performing implementation of the KM estimator to calculate several survival curves in a vectorized fashion, including upper and lower confidence intervals based on the Exponential Greenwood formula.</p> <p>However, this method can be very expensive at scales of hundreds of thousands of samples, due to the nearest neighbor search, both on training (construction of search index) and scoring (actual search).</p>"},{"location":"how_xgbse_works.html#xgbsekaplantree-single-tree-and-kaplan-meier-on-its-leaves","title":"<code>XGBSEKaplanTree</code>: single tree, and Kaplan-Meier on its leaves","text":"<p>As a simplification to <code>XGBSEKaplanNeighbors</code>, we also provide a single tree implementation. Instead of doing expensive nearest neighbor searches, we fit a single tree via <code>xgboost</code> and calculate KM curves at each of its leaves.</p> <p></p> <p>It is by far the most efficient implementation, able to scale to millions of examples easily. At fit time, the tree is built and all KM curves are pre-calculated, so that at scoring time a simple query will suffice to get the model's estimates.</p> <p>However, as we're fitting a single tree, predictive power may be worse. That could be a sensible tradeoff, but we also provide <code>XGBSEBootstrapEstimator</code>, a bootstrap abstraction where we can fit a forest of <code>XGBSEKaplanTree</code>'s to improve accuracy and reduce variance.</p>"},{"location":"how_xgbse_works.html#does-it-solve-the-problem","title":"Does it solve the problem?","text":"<p>Now we return to the first example and check how <code>XGBEmbedKaplanNeighbors</code> performs:</p> <pre><code># importing xgbse\nfrom xgbse import XGBSEKaplanNeighbors\nfrom xgbse._kaplan_neighbors import DEFAULT_PARAMS\nfrom xgbse.metrics import concordance_index\n\n# dummy function representing dataprep\ndtrain, dval = prepare_data()\n\n# loop to show different scale results\nfor scale in [1.5, 1.0, 0.5]:\n\n    # chaning parameter\n    DEFAULT_PARAMS['aft_loss_distribution_scale'] = scale\n\n    # training model\n    xgbse_model = XGBSEKaplanNeighbors(DEFAULT_PARAMS, n_neighbors=30)\n    xgbse_model.fit(\n        X_train, y_train,\n        validation_data = (X_valid, y_valid),\n        early_stopping_rounds=10,\n        time_bins=TIME_BINS\n    )\n\n    # predicting and evaluating\n    preds = xgbse_model.predict(X_valid)\n    cind = concordance_index(y_valid, preds)\n    avg_probs = preds[[30, 90, 150]].mean().values.round(4).tolist()\n\n    print(f\"aft_loss_distribution_scale: {scale}\")\n    print(f\"C-index: {cind:.3f}\")\n    print(f\"Average probability of survival at [30, 90, 150] days: {avg_probs}\")\n    print(\"----\")</code></pre> <pre><code>aft_loss_distribution_scale: 1.5\nC-index: 0.640\nAverage probability of survival at [30, 90, 150] days: [0.9109, 0.6854, 0.528]\n----\naft_loss_distribution_scale: 1.0\nC-index: 0.644\nAverage probability of survival at [30, 90, 150] days: [0.9111, 0.6889, 0.5333]\n----\naft_loss_distribution_scale: 0.5\nC-index: 0.650\nAverage probability of survival at [30, 90, 150] days: [0.913, 0.6904, 0.5289]\n----</code></pre> <p>As measured by the average probability of survival in 30, 90 and 150 days the model is very stable, showing similar calibration results independently of <code>aft_loss_distribution_scale</code> choice, with comparable (or a bit worse) C-index results. Visually, the comparison of the average model predictions to a Kaplan Meier yields much better results:</p> <p> <sub>XGBSE outputs a full survival curve and it is much more stable than vanilla XGBoost, its average prediction being fairly close to the unbiased KM estimator. </sub> </p> <p>No more point estimates and high variation! Although is too harsh to claim that the problem is solved, we believe that the package can be a good, more statistically robust alternative to survival analysis.</p>"},{"location":"how_xgbse_works.html#references","title":"References","text":"<p>[1] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, and J. Q. Candela. Practical Lessons from Predicting Clicks on Ads at Facebook (2014). In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising (ADKDD\u201914).</p> <p>[2] Feature transformations with ensembles of trees. Scikit-learn documentation at https://scikit-learn.org/.</p> <p>[3] G. Marmerola. Calibration of probabilities for tree-based models. Personal Blog at https://gdmarmerola.github.io/.</p> <p>[4] G. Marmerola. Supervised dimensionality reduction and clustering at scale with RFs with UMAP. Personal Blog at https://gdmarmerola.github.io/.</p> <p>[5] C. Yu, R. Greiner, H. Lin, V. Baracos. Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors. Advances in Neural Information Processing Systems 24 (NIPS 2011).</p> <p>[6] H. Kvamme, \u00d8. Borgan. The Brier Score under Administrative Censoring: Problems and Solutions. arXiv preprint arXiv:1912.08581.</p> <p>[7] S. Sawyer. The Greenwood and Exponential Greenwood Confidence Intervals in Survival Analysis. Handout on Washington University in St. Louis website.</p> <p>[8] H. Kvamme, \u00d8. Borgan, and I. Scheel. Time-to-event prediction with neural networks and Cox regression. Journal of Machine Learning Research, 20(129):1\u201330, 2019.</p> <p>[9] H. Haider, B. Hoehn, S. Davis, R. Greiner. Effective Ways to Build and Evaluate Individual Survival Distributions. Journal of Machine Learning Research 21 (2020) 1\u201363.</p>"},{"location":"install.html","title":"Install","text":"<p>You can easily install <code>xgbse</code> via pip:</p> <pre><code>pip install xgbse</code></pre>"},{"location":"benchmarks/benchmarks.html","title":"Benchmarks","text":""},{"location":"benchmarks/benchmarks.html#metrics","title":"Metrics","text":"<p>In the examples folder you'll find benchmarks comparing <code>xgbse</code> to other survival analysis methods. We show 6 metrics (see [9] for details):</p> <ul> <li><code>c-index</code>: concordance index. Equivalent to AUC with censored data.</li> <li><code>dcal_max_dev</code>: maximum decile deviation from calibrated distribution.</li> <li><code>dcal_pval</code>: p-value from chi-square test checking for D-Calibration. If larger than 0.05 then the model is D-Calibrated.</li> <li><code>ibs</code>: approximate integrated brier score, the average brier score across all time windows.</li> <li><code>inference_time</code>: time to perform inference, recorded on a 2018 MacBook Pro.</li> <li><code>training_time</code>: time to perform training, recorded on a 2018 MacBook Pro.</li> </ul> <p>We executed all methods with default parameters. For vanilla XGBoost and <code>xgbse</code>, early stopping was used, with <code>num_boosting_rounds=1000</code>, and  <code>early_stopping_rounds=10</code>. We show results below for five datasets.</p>"},{"location":"benchmarks/benchmarks.html#results","title":"Results","text":""},{"location":"benchmarks/benchmarks.html#flchain","title":"FLCHAIN","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time Weibull AFT 0.789 0.013 0.849 0.099 0.006 0.537 Cox-PH 0.788 0.011 0.971 0.099 0.005 0.942 XGBSE - Debiased BCE 0.784 0.037 0 0.117 0.233 3.062 XGBSE - Bootstrap Trees 0.781 0.009 0.985 0.1 0.382 15.351 XGBSE - Kaplan Neighbors 0.777 0.013 0.918 0.102 0.543 0.479 XGBSE - Stacked Weibull 0.776 0.008 0.994 0.103 0.011 0.719 XGB - Cox 0.775 nan nan nan 0.001 0.054 XGB - AFT 0.772 nan nan nan 0.001 0.106 XGBSE - Kaplan Tree 0.768 0.011 0.929 0.103 0.003 0.167"},{"location":"benchmarks/benchmarks.html#metabric","title":"METABRIC","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Stacked Weibull 0.63 0.045 0.146 0.162 0.01 0.525 XGBSE - Debiased BCE 0.627 0.033 0.128 0.165 0.09 3.165 XGBSE - Bootstrap Trees 0.624 0.024 0.563 0.155 0.301 6.165 Weibull AFT 0.622 0.024 0.667 0.154 0.005 0.284 Cox-PH 0.622 0.026 0.567 0.154 0.004 0.244 XGB - Cox 0.617 nan nan nan 0.001 0.096 XGBSE - Kaplan Neighbors 0.605 0.023 0.588 0.163 0.111 0.154 XGB - AFT 0.6 nan nan nan 0.001 0.044 XGBSE - Kaplan Tree 0.59 0.036 0.18 0.165 0.002 0.05"},{"location":"benchmarks/benchmarks.html#rrnlnph","title":"RRNLNPH","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Stacked Weibull 0.826 0.05 0 0.113 0.019 2.255 XGBSE - Bootstrap Trees 0.826 0.035 0 0.097 0.534 44.736 XGBSE - Kaplan Neighbors 0.824 0.038 0 0.1 15.662 1.504 XGBSE - Debiased BCE 0.824 0.068 0 0.108 0.285 4.562 XGB - Cox 0.824 nan nan nan 0.002 0.375 XGB - AFT 0.823 nan nan nan 0.001 0.243 XGBSE - Kaplan Tree 0.821 0.044 0 0.101 0.006 0.49 Weibull AFT 0.787 0.057 0 0.136 0.01 0.326 Cox-PH 0.787 0.055 0 0.135 0.021 2.267"},{"location":"benchmarks/benchmarks.html#sac3","title":"SAC3","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Stacked Weibull 0.697 0.04 0 0.171 0.153 32.469 XGB - AFT 0.691 nan nan nan 0.004 7.413 XGBSE - Debiased BCE 0.69 0.045 0 0.169 1.141 47.814 XGB - Cox 0.686 nan nan nan 0.002 4.885 Cox-PH 0.682 0.035 0 0.165 0.039 1.84 Weibull AFT 0.682 0.039 0 0.165 0.043 2.307 XGBSE - Bootstrap Trees 0.677 0.043 0 0.168 3.134 164.173 XGBSE - Kaplan Neighbors 0.666 0.037 0 0.175 416.382 36.759 XGBSE - Kaplan Tree 0.631 0.034 0 0.191 0.036 1.478"},{"location":"benchmarks/benchmarks.html#support","title":"SUPPORT","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Stacked Weibull 0.621 0.092 0 0.198 0.013 0.954 XGBSE - Debiased BCE 0.617 0.139 0 0.188 0.272 3.852 XGB - Cox 0.61 nan nan nan 0.001 0.069 XGB - AFT 0.609 nan nan nan 0.001 0.137 XGBSE - Bootstrap Trees 0.607 0.103 0 0.188 0.371 18.202 XGBSE - Kaplan Neighbors 0.601 0.099 0 0.197 1.488 0.752 XGBSE - Kaplan Tree 0.598 0.097 0 0.203 0.004 0.149 Cox-PH 0.578 0.16 0 0.201 0.006 0.465 Weibull AFT 0.576 0.138 0 0.201 0.007 0.461"},{"location":"benchmarks/benchmarks.html#analysis","title":"Analysis","text":"<ul> <li><code>XGBSEDebiasedBCE</code> and <code>XGBSEStackedWeibull</code> show the most promising results, being in the top three methods 4 out of 5 times.</li> <li>Other <code>xgbse</code> methods show good results too. In particular <code>XGBSEKaplanTree</code> with <code>XGBSEBootstrapEstimator</code> shows promising results, pointing to a direction for further research.</li> <li>Linear methods such as the Weibull AFT and Cox-PH from <code>lifelines</code> are surprisingly strong, specially for datasets with a small number of samples.</li> <li><code>xgbse</code> methods show competitive results to vanilla <code>xgboost</code> as measured by C-index, while showing good results for \"survival curve metrics\". Thus, we can use <code>xgbse</code> as a calibrated replacement to vanilla <code>xgboost</code>.</li> <li><code>xgbse</code> takes longer to fit than vanilla <code>xgboost</code>. Specially for <code>XGBSEDebiasedBCE</code>, we have to build N logistic regressions where N is the number of time windows we'll predict. In all cases we used N = 30. <code>XGBSEStackedWeibull</code> is the most efficient method, behind <code>XGBSEKaplanTree</code>.</li> </ul>"},{"location":"examples/basic_usage.html","title":"Basic Usage:","text":"<p>In this notebook you will find: - How to get a survival curve and neighbors prediction using xgbse - How to validate your xgbse model using sklearn</p>"},{"location":"examples/basic_usage.html#metrabic","title":"Metrabic","text":"<p>We will be using the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) dataset from pycox as base for this example.</p> <pre><code>from xgbse.converters import convert_to_structured\nfrom pycox.datasets import metabric\nimport numpy as np\n\n# getting data\ndf = metabric.read_df()\n\ndf.head()</code></pre> x0 x1 x2 x3 x4 x5 x6 x7 x8 duration event 0 5.603834 7.811392 10.797988 5.967607 1.0 1.0 0.0 1.0 56.840000 99.333336 0 1 5.284882 9.581043 10.204620 5.664970 1.0 0.0 0.0 1.0 85.940002 95.733330 1 2 5.920251 6.776564 12.431715 5.873857 0.0 1.0 0.0 1.0 48.439999 140.233337 0 3 6.654017 5.341846 8.646379 5.655888 0.0 0.0 0.0 0.0 66.910004 239.300003 0 4 5.456747 5.339741 10.555724 6.008429 1.0 0.0 0.0 1.0 67.849998 56.933334 1"},{"location":"examples/basic_usage.html#split-and-time-bins","title":"Split and Time Bins","text":"<p>Split the data in train and test, using sklearn API. We also setup the TIME_BINS array, which will be used to fit the survival curve</p> <pre><code>from xgbse.converters import convert_to_structured\nfrom sklearn.model_selection import train_test_split\n\n# splitting to X, T, E format\nX = df.drop(['duration', 'event'], axis=1)\nT = df['duration']\nE = df['event']\ny = convert_to_structured(T, E)\n\n# splitting between train, and validation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state = 0)\nTIME_BINS = np.arange(15, 315, 15)\nTIME_BINS</code></pre> <pre><code>array([ 15,  30,  45,  60,  75,  90, 105, 120, 135, 150, 165, 180, 195,\n       210, 225, 240, 255, 270, 285, 300])</code></pre>"},{"location":"examples/basic_usage.html#fit-and-predict","title":"Fit and Predict","text":"<p>We will be using the DebiasedBCE estimator to fit the model and predict a survival curve for each point in our test data</p> <pre><code>from xgbse import XGBSEDebiasedBCE\n\n# fitting xgbse model\nxgbse_model = XGBSEDebiasedBCE()\nxgbse_model.fit(X_train, y_train, time_bins=TIME_BINS)\n\n# predicting\ny_pred = xgbse_model.predict(X_test)\n\nprint(y_pred.shape)\ny_pred.head()</code></pre> <pre><code>(635, 20)</code></pre> 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 0 0.983502 0.951852 0.923277 0.900028 0.862270 0.799324 0.715860 0.687257 0.651314 0.610916 0.568001 0.513172 0.493194 0.430701 0.377675 0.310496 0.272169 0.225599 0.184878 0.144089 1 0.973506 0.917739 0.839154 0.710431 0.663119 0.558886 0.495204 0.364995 0.311628 0.299939 0.226226 0.191373 0.171697 0.144864 0.112447 0.089558 0.081137 0.057679 0.048563 0.035985 2 0.986894 0.959209 0.919768 0.889910 0.853239 0.777208 0.725381 0.649177 0.582569 0.531787 0.485275 0.451667 0.428899 0.386413 0.344369 0.279685 0.242064 0.187967 0.158121 0.118562 3 0.986753 0.955210 0.910354 0.857684 0.824301 0.769262 0.665805 0.624934 0.583592 0.537261 0.493957 0.443193 0.416702 0.376552 0.308947 0.237033 0.177140 0.141838 0.117917 0.088937 4 0.977348 0.940368 0.873695 0.804796 0.742655 0.632426 0.556008 0.521490 0.493577 0.458477 0.416363 0.391099 0.364431 0.291472 0.223758 0.190398 0.165911 0.120061 0.095512 0.069566 <p>mean predicted survival curve for test data</p> <pre><code>y_pred.mean().plot.line();</code></pre> <p></p>"},{"location":"examples/basic_usage.html#neighbors","title":"Neighbors","text":"<p>We can also use our model for querying comparables based on survivability.</p> <pre><code>neighbors = xgbse_model.get_neighbors(\n    query_data = X_test,\n    index_data = X_train,\n    n_neighbors = 5\n)\n\nprint(neighbors.shape)\nneighbors.head(5)</code></pre> <pre><code>(635, 5)</code></pre> neighbor_1 neighbor_2 neighbor_3 neighbor_4 neighbor_5 829 339 166 508 1879 418 670 1846 1082 1297 194 1448 1064 416 1230 739 1392 589 85 1558 8 1080 613 1522 1814 105 859 1743 50 566 <p>example: selecting a data point from query data (X_test) and checking its features</p> <pre><code>desired = neighbors.iloc[10]\n\nX_test.loc[X_test.index == desired.name]</code></pre> x0 x1 x2 x3 x4 x5 x6 x7 x8 399 5.572504 7.367552 11.023443 5.406307 1.0 0.0 0.0 1.0 67.620003 <p>... and finding its comparables from index data (X_train)</p> <pre><code>X_train.loc[X_train.index.isin(desired.tolist())]</code></pre> x0 x1 x2 x3 x4 x5 x6 x7 x8 757 5.745395 8.178815 10.745699 5.530381 1.0 1.0 0.0 1.0 64.930000 726 5.635854 6.648942 10.889588 5.496374 1.0 1.0 0.0 1.0 70.860001 968 5.541239 7.058089 10.463409 5.396433 1.0 0.0 0.0 1.0 71.070000 870 5.605712 7.309217 10.935708 5.542732 0.0 1.0 0.0 1.0 71.470001 1640 5.812605 7.646811 10.952687 5.516386 1.0 1.0 0.0 1.0 68.559998"},{"location":"examples/basic_usage.html#score-metrics","title":"Score metrics","text":"<p>XGBSE implements concordance index and integrated brier score, both can be used to evaluate model performance</p> <pre><code># importing metrics\nfrom xgbse.metrics import concordance_index, approx_brier_score\n\n# running metrics\nprint(f\"C-index: {concordance_index(y_test, y_pred)}\")\nprint(f\"Avg. Brier Score: {approx_brier_score(y_test, y_pred)}\")</code></pre> <pre><code>C-index: 0.6706453426714781\nAvg. Brier Score: 0.17221909077845754</code></pre>"},{"location":"examples/basic_usage.html#cross-validation","title":"Cross Validation","text":"<p>We can also use sklearn's cross_val_score and make_scorer to cross validate our model</p> <pre><code>from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nresults = cross_val_score(xgbse_model, X, y, scoring=make_scorer(approx_brier_score))\nresults</code></pre> <pre><code>array([0.16269636, 0.14880423, 0.12848939, 0.15335356, 0.15394174])</code></pre>"},{"location":"examples/confidence_interval.html","title":"Confidence Interval:","text":"<p>In this notebook you will find: - Get confidence intervals for predicted survival curves using XGBSE estimators; - How to use XGBSEBootstrapEstimator, a meta estimator for bagging; - A nice function to help us plot survival curves.</p> <pre><code>import matplotlib.pyplot as plt\nplt.style.use('bmh')\n\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('retina')\n\n# to easily plot confidence intervals\ndef plot_ci(mean, upper_ci, lower_ci, i=42, title='Probability of survival $P(T \\geq t)$'):\n\n    # plotting mean and confidence intervals\n    plt.figure(figsize=(12, 4), dpi=120)\n    plt.plot(mean.columns,mean.iloc[i])\n    plt.fill_between(mean.columns, lower_ci.iloc[i], upper_ci.iloc[i], alpha=0.2)\n\n    plt.title(title)\n    plt.xlabel('Time [days]')\n    plt.ylabel('Probability')\n    plt.tight_layout()\n</code></pre>"},{"location":"examples/confidence_interval.html#metrabic","title":"Metrabic","text":"<p>We will be using the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) dataset from pycox as base for this example.</p> <pre><code>from xgbse.converters import convert_to_structured\nfrom pycox.datasets import metabric\nimport numpy as np\n\n# getting data\ndf = metabric.read_df()\n\ndf.head()</code></pre> x0 x1 x2 x3 x4 x5 x6 x7 x8 duration event 0 5.603834 7.811392 10.797988 5.967607 1.0 1.0 0.0 1.0 56.840000 99.333336 0 1 5.284882 9.581043 10.204620 5.664970 1.0 0.0 0.0 1.0 85.940002 95.733330 1 2 5.920251 6.776564 12.431715 5.873857 0.0 1.0 0.0 1.0 48.439999 140.233337 0 3 6.654017 5.341846 8.646379 5.655888 0.0 0.0 0.0 0.0 66.910004 239.300003 0 4 5.456747 5.339741 10.555724 6.008429 1.0 0.0 0.0 1.0 67.849998 56.933334 1"},{"location":"examples/confidence_interval.html#split-and-time-bins","title":"Split and Time Bins","text":"<p>Split the data in train and test, using sklearn API. We also setup the TIME_BINS array, which will be used to fit the survival curve.</p> <pre><code>from xgbse.converters import convert_to_structured\nfrom sklearn.model_selection import train_test_split\n\n# splitting to X, T, E format\nX = df.drop(['duration', 'event'], axis=1)\nT = df['duration']\nE = df['event']\ny = convert_to_structured(T, E)\n\n# splitting between train, and validation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state = 0)\nTIME_BINS = np.arange(15, 315, 15)\nTIME_BINS</code></pre> <pre><code>array([ 15,  30,  45,  60,  75,  90, 105, 120, 135, 150, 165, 180, 195,\n       210, 225, 240, 255, 270, 285, 300])</code></pre>"},{"location":"examples/confidence_interval.html#calculating-confidence-intervals","title":"Calculating confidence intervals","text":"<p>We will be using the XGBSEKaplanTree estimator to fit the model and predict a survival curve for each point in our test data, and via return_ci parameter we will get upper and lower bounds for the confidence interval.</p> <pre><code>from xgbse import XGBSEKaplanTree, XGBSEBootstrapEstimator\nfrom xgbse.metrics import concordance_index, approx_brier_score\n\n# xgboost parameters to fit our model\nPARAMS_TREE = {\n    'objective': 'survival:cox',\n    'eval_metric': 'cox-nloglik',\n    'tree_method': 'hist',\n    'max_depth': 10,\n    'booster':'dart',\n    'subsample': 1.0,\n    'min_child_weight': 50,\n    'colsample_bynode': 1.0\n}</code></pre>"},{"location":"examples/confidence_interval.html#numerical-form","title":"Numerical Form","text":"<p>The KaplanTree and KaplanNeighbors models support estimation of confidence intervals via the Exponential Greenwood formula.</p> <pre><code>%%time\n\n# fitting xgbse model\nxgbse_model = XGBSEKaplanTree(PARAMS_TREE)\nxgbse_model.fit(X_train, y_train, time_bins=TIME_BINS)\n\n# predicting\nmean, upper_ci, lower_ci = xgbse_model.predict(X_test, return_ci=True)\n\n# print metrics\nprint(f\"C-index: {concordance_index(y_test, mean)}\")\nprint(f\"Avg. Brier Score: {approx_brier_score(y_test, mean)}\")\n\n# plotting CIs\nplot_ci(mean, upper_ci, lower_ci)</code></pre> <pre><code>C-index: 0.6358942056527093\nAvg. Brier Score: 0.182841148106733\nCPU times: user 1.88 s, sys: 9.37 ms, total: 1.88 s\nWall time: 897 ms</code></pre> <p></p>"},{"location":"examples/confidence_interval.html#non-parametric-form","title":"Non-parametric Form","text":"<p>We can also use the XGBSEBootstrapEstimator to wrap any XGBSE model and get confidence intervals via bagging, which also slighty increase our performance at the cost of computation time.</p> <pre><code>%%time\n\n# base model as XGBSEKaplanTree\nbase_model = XGBSEKaplanTree(PARAMS_TREE)\n\n# bootstrap meta estimator\nbootstrap_estimator = XGBSEBootstrapEstimator(base_model, n_estimators=100)\n\n# fitting the meta estimator\nbootstrap_estimator.fit(X_train, y_train, time_bins=TIME_BINS)\n\n# predicting\nmean, upper_ci, lower_ci = bootstrap_estimator.predict(X_test, return_ci=True)\n\n# print metrics\nprint(f\"C-index: {concordance_index(y_test, mean)}\")\nprint(f\"Avg. Brier Score: {approx_brier_score(y_test, mean)}\")\n\n# plotting CIs\nplot_ci(mean, upper_ci, lower_ci)</code></pre> <pre><code>C-index: 0.6580651819585904\nAvg. Brier Score: 0.17040560738276272\nCPU times: user 17.3 s, sys: 58.5 ms, total: 17.4 s\nWall time: 4.18 s</code></pre> <p></p> <pre><code></code></pre> <pre><code></code></pre>"},{"location":"examples/extrapolation_example.html","title":"Extrapolation","text":"<p>In this notebook you will find: - How to get a survival curve using xgbse - How to extrapolate your predicted survival curve using the <code>xgbse.extrapolation</code> module</p>"},{"location":"examples/extrapolation_example.html#metrabic","title":"Metrabic","text":"<p>We will be using the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) dataset from pycox as base for this example.</p> <pre><code>from xgbse.converters import convert_to_structured\nfrom pycox.datasets import metabric\nimport numpy as np\n\n# getting data\ndf = metabric.read_df()\n\ndf.head()</code></pre> x0 x1 x2 x3 x4 x5 x6 x7 x8 duration event 0 5.603834 7.811392 10.797988 5.967607 1.0 1.0 0.0 1.0 56.840000 99.333336 0 1 5.284882 9.581043 10.204620 5.664970 1.0 0.0 0.0 1.0 85.940002 95.733330 1 2 5.920251 6.776564 12.431715 5.873857 0.0 1.0 0.0 1.0 48.439999 140.233337 0 3 6.654017 5.341846 8.646379 5.655888 0.0 0.0 0.0 0.0 66.910004 239.300003 0 4 5.456747 5.339741 10.555724 6.008429 1.0 0.0 0.0 1.0 67.849998 56.933334 1"},{"location":"examples/extrapolation_example.html#split-and-create-time-bins","title":"Split and create Time Bins","text":"<p>Split the data in train and test, using sklearn API. We also setup the TIME_BINS arange, which will be used to fit the survival curve</p> <pre><code>from xgbse.converters import convert_to_structured\nfrom sklearn.model_selection import train_test_split\n\n# splitting to X, T, E format\nX = df.drop(['duration', 'event'], axis=1)\nT = df['duration']\nE = df['event']\ny = convert_to_structured(T, E)\n\n# splitting between train, and validation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state = 0)\nTIME_BINS = np.arange(15, 315, 15)\nTIME_BINS</code></pre> <pre><code>array([ 15,  30,  45,  60,  75,  90, 105, 120, 135, 150, 165, 180, 195,\n       210, 225, 240, 255, 270, 285, 300])</code></pre>"},{"location":"examples/extrapolation_example.html#fit-model-and-predict-survival-curves","title":"Fit model and predict survival curves","text":"<p>The package follows <code>scikit-learn</code> API, with a minor adaptation to work with time and event data. The model outputs the probability of survival, in a <code>pd.Dataframe</code> where columns represent different times.</p> <pre><code>from xgbse import XGBSEDebiasedBCE\n\n# fitting xgbse model\nxgbse_model = XGBSEDebiasedBCE()\nxgbse_model.fit(X_train, y_train, time_bins=TIME_BINS)\n\n# predicting\nsurvival = xgbse_model.predict(X_test)\nsurvival.head()</code></pre> 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 0 0.983502 0.951852 0.923277 0.900028 0.862270 0.799324 0.715860 0.687257 0.651314 0.610916 0.568001 0.513172 0.493194 0.430701 0.377675 0.310496 0.272169 0.225599 0.184878 0.144089 1 0.973506 0.917739 0.839154 0.710431 0.663119 0.558886 0.495204 0.364995 0.311628 0.299939 0.226226 0.191373 0.171697 0.144864 0.112447 0.089558 0.081137 0.057679 0.048563 0.035985 2 0.986894 0.959209 0.919768 0.889910 0.853239 0.777208 0.725381 0.649177 0.582569 0.531787 0.485275 0.451667 0.428899 0.386413 0.344369 0.279685 0.242064 0.187967 0.158121 0.118562 3 0.986753 0.955210 0.910354 0.857684 0.824301 0.769262 0.665805 0.624934 0.583592 0.537261 0.493957 0.443193 0.416702 0.376552 0.308947 0.237033 0.177140 0.141838 0.117917 0.088937 4 0.977348 0.940368 0.873695 0.804796 0.742655 0.632426 0.556008 0.521490 0.493577 0.458477 0.416363 0.391099 0.364431 0.291472 0.223758 0.190398 0.165911 0.120061 0.095512 0.069566"},{"location":"examples/extrapolation_example.html#survival-curves-visualization","title":"Survival curves visualization","text":"<pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,4), dpi=120)\n\nplt.plot(\n    survival.columns,\n    survival.iloc[42],\n    'k--',\n    label='Survival'\n)\n\nplt.title('Sample of predicted survival curves - $P(T&gt;t)$')\nplt.legend()</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7fc38026cef0&gt;</code></pre> <p>Notice that this predicted survival curve does not end at zero (cure fraction due to censored data). In some cases it might be useful to extrapolate our survival curves using specific strategies. <code>xgbse.extrapolation</code> implements a constant risk extrapolation strategy.</p>"},{"location":"examples/extrapolation_example.html#extrapolation_1","title":"Extrapolation","text":"<pre><code>from xgbse.extrapolation import extrapolate_constant_risk\n\n# extrapolating predicted survival\nsurvival_ext = extrapolate_constant_risk(survival, 450, 15)\nsurvival_ext.head()</code></pre> 15.0 30.0 45.0 60.0 75.0 90.0 105.0 120.0 135.0 150.0 ... 315.0 330.0 345.0 360.0 375.0 390.0 405.0 420.0 435.0 450.0 0 0.983502 0.951852 0.923277 0.900028 0.862270 0.799324 0.715860 0.687257 0.651314 0.610916 ... 0.112299 0.068213 0.032292 0.011915 0.003426 0.000768 0.000134 1.825794e-05 1.937124e-06 1.601799e-07 1 0.973506 0.917739 0.839154 0.710431 0.663119 0.558886 0.495204 0.364995 0.311628 0.299939 ... 0.026665 0.014641 0.005957 0.001796 0.000401 0.000066 0.000008 7.404100e-07 4.986652e-08 2.488634e-09 2 0.986894 0.959209 0.919768 0.889910 0.853239 0.777208 0.725381 0.649177 0.582569 0.531787 ... 0.088900 0.049982 0.021071 0.006660 0.001579 0.000281 0.000037 3.735612e-06 2.798762e-07 1.572266e-08 3 0.986753 0.955210 0.910354 0.857684 0.824301 0.769262 0.665805 0.624934 0.583592 0.537261 ... 0.067080 0.038160 0.016373 0.005299 0.001293 0.000238 0.000033 3.462388e-06 2.734946e-07 1.629408e-08 4 0.977348 0.940368 0.873695 0.804796 0.742655 0.632426 0.556008 0.521490 0.493577 0.458477 ... 0.050668 0.026879 0.010385 0.002923 0.000599 0.000089 0.000010 7.701555e-07 4.442463e-08 1.866412e-09 <p>5 rows \u00d7 31 columns</p> <pre><code># plotting extrapolation #\n\nplt.figure(figsize=(12,4), dpi=120)\n\nplt.plot(\n    survival.columns,\n    survival.iloc[42],\n    'k--',\n    label='Survival'\n)\n\nplt.plot(\n    survival_ext.columns,\n    survival_ext.iloc[42],\n    'tomato',\n    alpha=0.5,\n    label='Extrapolated Survival'\n)\n\nplt.title('Extrapolation of survival curves')\nplt.legend()</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7fc3801842b0&gt;</code></pre> <p></p>"},{"location":"modules/base.html","title":"xgbse._base.XGBSEBaseEstimator","text":"<p>Base class for all estimators in xgbse. Implements explainability through prototypes</p> Source code in <code>xgbse/_base.py</code> <pre><code>class XGBSEBaseEstimator(BaseEstimator):\n    \"\"\"\n    Base class for all estimators in xgbse. Implements explainability through prototypes\n    \"\"\"\n\n    def __init__(\n        self,\n        xgb_params: Optional[Dict[str, Any]] = None,\n        enable_categorical: bool = False,\n    ):\n        self.feature_extractor = FeatureExtractor(\n            xgb_params=xgb_params, enable_categorical=enable_categorical\n        )\n        self.xgb_params = self.feature_extractor.xgb_params\n\n        self.feature_importances_ = None\n        self.persist_train = False\n\n        self.index_id = None\n        self.tree = None\n\n    def fit_feature_extractor(\n        self,\n        X,\n        y,\n        time_bins: Optional[Sequence] = None,\n        validation_data: Optional[List[Tuple[Any, Any]]] = None,\n        num_boost_round: int = 10,\n        early_stopping_rounds: Optional[int] = None,\n        verbose_eval: int = 0,\n    ):\n        self.feature_extractor.fit(\n            X,\n            y,\n            time_bins=time_bins,\n            validation_data=validation_data,\n            num_boost_round=num_boost_round,\n            early_stopping_rounds=early_stopping_rounds,\n            verbose_eval=verbose_eval,\n        )\n        self.feature_importances_ = self.feature_extractor.feature_importances_\n        self.time_bins = self.feature_extractor.time_bins\n\n    def get_neighbors(\n        self,\n        query_data,\n        index_data=None,\n        query_id=None,\n        index_id=None,\n        n_neighbors: int = 30,\n    ):\n        \"\"\"\n        Search for portotypes (size: n_neighbors) for each unit in a\n        dataframe X. If units array is specified, comparables will be returned using\n        its identifiers. If not, a dataframe of comparables indexes for each sample\n        in X is returned.\n\n        Args:\n            query_data (pd.DataFrame): Dataframe of features to be used as input\n\n            query_id ([pd.Series, np.array]): Series or array of identification for\n                each sample of query_data. Will be used in set_index if specified.\n\n            index_id ([pd.Series, np.array]): Series or array of identification for\n                each sample of index_id.\n                If specified, comparables will be returned using this identifier.\n\n            n_neighbors (int): Number of neighbors/comparables to be considered.\n\n        Returns:\n            comps_df (pd.DataFrame): A dataframe of comparables/neighbors for each\n            evaluated sample. If units identifier is specified, the output dataframe\n            is converted to use units the proper identifier for each sample. The\n            reference sample is considered to be the index of the dataframe and\n            its comparables are its specific row values.\n        \"\"\"\n\n        if index_data is None and not self.persist_train:\n            raise ValueError(\"please specify the index_data\")\n\n        if index_id is None and not self.persist_train:\n            index_id = index_data.index.copy()\n\n        if query_id is None:\n            query_id = query_data.index.copy()\n\n        if self.persist_train:\n            index_id = self.index_id\n            index = self.tree\n        else:\n            index_leaves = self.feature_extractor.predict_leaves(index_data)\n\n            if len(index_leaves.shape) == 1:\n                index_leaves = index_leaves.reshape(-1, 1)\n            index = BallTree(index_leaves, metric=\"hamming\")\n\n        query_leaves = self.feature_extractor.predict_leaves(query_data)\n\n        if len(query_leaves.shape) == 1:\n            query_leaves = query_leaves.reshape(-1, 1)\n        compset = index.query(query_leaves, k=n_neighbors + 1, return_distance=False)\n\n        map_to_id = np.vectorize(lambda x: index_id[x])\n        comparables = map_to_id(compset)\n        comps_df = pd.DataFrame(comparables[:, 1:]).set_index(query_id)\n        comps_df.columns = [f\"neighbor_{n + 1}\" for n in comps_df.columns]\n\n        return comps_df\n</code></pre>"},{"location":"modules/base.html#xgbse._base.XGBSEBaseEstimator.get_neighbors","title":"<code>get_neighbors(self, query_data, index_data=None, query_id=None, index_id=None, n_neighbors=30)</code>","text":"<p>Search for portotypes (size: n_neighbors) for each unit in a dataframe X. If units array is specified, comparables will be returned using its identifiers. If not, a dataframe of comparables indexes for each sample in X is returned.</p> <p>Parameters:</p> Name Type Description Default <code>query_data</code> <code>pd.DataFrame</code> <p>Dataframe of features to be used as input</p> required <code>query_id</code> <code>[pd.Series, np.array]</code> <p>Series or array of identification for each sample of query_data. Will be used in set_index if specified.</p> <code>None</code> <code>index_id</code> <code>[pd.Series, np.array]</code> <p>Series or array of identification for each sample of index_id. If specified, comparables will be returned using this identifier.</p> <code>None</code> <code>n_neighbors</code> <code>int</code> <p>Number of neighbors/comparables to be considered.</p> <code>30</code> <p>Returns:</p> Type Description <code>comps_df (pd.DataFrame)</code> <p>A dataframe of comparables/neighbors for each evaluated sample. If units identifier is specified, the output dataframe is converted to use units the proper identifier for each sample. The reference sample is considered to be the index of the dataframe and its comparables are its specific row values.</p> Source code in <code>xgbse/_base.py</code> <pre><code>def get_neighbors(\n    self,\n    query_data,\n    index_data=None,\n    query_id=None,\n    index_id=None,\n    n_neighbors: int = 30,\n):\n    \"\"\"\n    Search for portotypes (size: n_neighbors) for each unit in a\n    dataframe X. If units array is specified, comparables will be returned using\n    its identifiers. If not, a dataframe of comparables indexes for each sample\n    in X is returned.\n\n    Args:\n        query_data (pd.DataFrame): Dataframe of features to be used as input\n\n        query_id ([pd.Series, np.array]): Series or array of identification for\n            each sample of query_data. Will be used in set_index if specified.\n\n        index_id ([pd.Series, np.array]): Series or array of identification for\n            each sample of index_id.\n            If specified, comparables will be returned using this identifier.\n\n        n_neighbors (int): Number of neighbors/comparables to be considered.\n\n    Returns:\n        comps_df (pd.DataFrame): A dataframe of comparables/neighbors for each\n        evaluated sample. If units identifier is specified, the output dataframe\n        is converted to use units the proper identifier for each sample. The\n        reference sample is considered to be the index of the dataframe and\n        its comparables are its specific row values.\n    \"\"\"\n\n    if index_data is None and not self.persist_train:\n        raise ValueError(\"please specify the index_data\")\n\n    if index_id is None and not self.persist_train:\n        index_id = index_data.index.copy()\n\n    if query_id is None:\n        query_id = query_data.index.copy()\n\n    if self.persist_train:\n        index_id = self.index_id\n        index = self.tree\n    else:\n        index_leaves = self.feature_extractor.predict_leaves(index_data)\n\n        if len(index_leaves.shape) == 1:\n            index_leaves = index_leaves.reshape(-1, 1)\n        index = BallTree(index_leaves, metric=\"hamming\")\n\n    query_leaves = self.feature_extractor.predict_leaves(query_data)\n\n    if len(query_leaves.shape) == 1:\n        query_leaves = query_leaves.reshape(-1, 1)\n    compset = index.query(query_leaves, k=n_neighbors + 1, return_distance=False)\n\n    map_to_id = np.vectorize(lambda x: index_id[x])\n    comparables = map_to_id(compset)\n    comps_df = pd.DataFrame(comparables[:, 1:]).set_index(query_id)\n    comps_df.columns = [f\"neighbor_{n + 1}\" for n in comps_df.columns]\n\n    return comps_df\n</code></pre>"},{"location":"modules/bce.html","title":"xgbse._debiased_bce.XGBSEDebiasedBCE","text":"<p>Train a set of logistic regressions on top of leaf embeddings produced by XGBoost, each predicting survival at different user-defined discrete time windows. The classifiers remove individuals as they are censored,  with targets that are indicatorsof surviving at each window.</p> <p>Note</p> <ul> <li>Training and scoring of logistic regression models is efficient, being performed in parallel through joblib, so the model can scale to hundreds of thousands or millions of samples.</li> <li>However, if many windows are used and data is large, training of logistic regression models may become a bottleneck, taking more time than training of the underlying XGBoost model.</li> </ul> <p>Read more in How XGBSE works.</p> Source code in <code>xgbse/_debiased_bce.py</code> <pre><code>class XGBSEDebiasedBCE(XGBSEBaseEstimator):\n    \"\"\"\n    Train a set of logistic regressions on top of leaf embeddings produced by XGBoost,\n    each predicting survival at different user-defined discrete time windows.\n    The classifiers remove individuals as they are censored,\n     with targets that are indicatorsof surviving at each window.\n\n    !!! Note\n        * Training and scoring of logistic regression models is efficient,\n        being performed in parallel through joblib, so the model can scale to\n        hundreds of thousands or millions of samples.\n        * However, if many windows are used and data is large, training of\n        logistic regression models may become a bottleneck, taking more time\n        than training of the underlying XGBoost model.\n\n    Read more in [How XGBSE works](https://loft-br.github.io/xgboost-survival-embeddings/how_xgbse_works.html).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        xgb_params: Optional[Dict[str, Any]] = None,\n        lr_params: Dict[str, Any] = {},\n        n_jobs: int = 1,\n        enable_categorical: bool = False,\n    ):\n        \"\"\"\n        Args:\n            xgb_params (Dict, None): Parameters for XGBoost model.\n                If None, will use XGBoost defaults and set objective as `survival:aft`.\n                Check &lt;https://xgboost.readthedocs.io/en/latest/parameter.html&gt; for options.\n\n            lr_params (Dict, None): Parameters for LogisticRegression model.\n                If None, will use scikit-learn default parameters.\n\n            n_jobs (int): Number of jobs used for parallel training of logistic regressions.\n\n            enable_categorical (bool): Enable categorical feature support on xgboost model\n        \"\"\"\n        super().__init__(xgb_params=xgb_params, enable_categorical=enable_categorical)\n        self.lr_params = lr_params\n        self.n_jobs = n_jobs\n\n    def fit(\n        self,\n        X,\n        y,\n        time_bins: Optional[Sequence] = None,\n        validation_data: Optional[List[Tuple[Any, Any]]] = None,\n        num_boost_round: int = 10,\n        early_stopping_rounds: Optional[int] = None,\n        verbose_eval: int = 0,\n        persist_train: bool = False,\n        index_id=None,\n    ):\n        \"\"\"\n        Transform feature space by fitting a XGBoost model and returning its leafs.\n        Leaves are transformed and considered as dummy variables to fit multiple\n        logistic regression models to each evaluated time bin.\n\n        Args:\n            X ([pd.DataFrame, np.array]): Features to be used while fitting XGBoost model\n\n            y (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n                and time of event or time of censoring as second field.\n\n            time_bins (np.array): Specified time windows to use when making survival predictions\n\n            validation_data (List[Tuple]): Validation data in the format of a list of tuples [(X, y)]\n                if user desires to use early stopping\n\n            num_boost_round (Int): Number of boosting iterations.\n\n            early_stopping_rounds (Int): Activates early stopping.\n                Validation metric needs to improve at least once\n                in every **early_stopping_rounds** round(s) to continue training.\n                See xgboost.train documentation.\n\n            verbose_eval ([Bool, Int]): Level of verbosity. See xgboost.train documentation.\n\n            persist_train (Bool): Whether or not to persist training data to use explainability\n                through prototypes\n\n            index_id (pd.Index): User defined index if intended to use explainability\n                through prototypes\n\n        Returns:\n            XGBSEDebiasedBCE: Trained XGBSEDebiasedBCE instance\n        \"\"\"\n        self.fit_feature_extractor(\n            X,\n            y,\n            time_bins=time_bins,\n            validation_data=validation_data,\n            num_boost_round=num_boost_round,\n            early_stopping_rounds=early_stopping_rounds,\n            verbose_eval=verbose_eval,\n        )\n\n        E_train, T_train = convert_y(y)\n        # predicting and encoding leaves\n        self.encoder = OneHotEncoder()\n        leaves = self.feature_extractor.predict_leaves(X)\n        leaves_encoded = self.encoder.fit_transform(leaves)\n\n        # convert targets for using with logistic regression\n        self.targets, self.time_bins = _build_multi_task_targets(\n            E_train, T_train, self.feature_extractor.time_bins\n        )\n\n        # fitting LR for several targets\n        self.lr_estimators_ = self._fit_all_lr(leaves_encoded, self.targets)\n\n        if persist_train:\n            self.persist_train = True\n            if index_id is None:\n                index_id = X.index.copy()\n            self.tree = BallTree(leaves, metric=\"hamming\")\n\n        self.index_id = index_id\n\n        return self\n\n    def _fit_one_lr(self, leaves_encoded, target):\n        \"\"\"\n        Fits a single logistic regression to predict survival probability\n        at a certain time bin as target. Encoded leaves are used as features.\n\n        Args:\n            leaves_encoded (np.array): A tensor of one hot encoded leaves.\n\n            target (np.array): An array of time targets for a specific\n\n        Returns:\n            lr (sklearn.linear_model.LogisticRegression): A fitted Logistic\n            Regression model. This model outputs calibrated survival probabilities\n            on a time T.\n        \"\"\"\n\n        # masking\n        mask = target != -1\n\n        # by default we use a logistic regression\n        classifier = LogisticRegression(**self.lr_params)\n\n        if len(target[mask]) == 0:\n            # If there's no observation in a time bucket we raise an error\n            raise ValueError(\"Error: No observations in a time bucket\")\n        elif len(np.unique(target[mask])) == 1:\n            # If there's only one class in a time bucket\n            # we create a dummy classifier that predicts that class and send a warning\n            warnings.warn(\n                \"Warning: Only one class found in a time bucket\", RuntimeWarning\n            )\n            classifier = DummyLogisticRegression()\n\n        classifier.fit(leaves_encoded[mask, :], target[mask])\n        return classifier\n\n    def _fit_all_lr(self, leaves_encoded, targets):\n        \"\"\"\n        Fits multiple Logistic Regressions to predict survival probability\n        for a list of time bins as target. Encoded leaves are used as features.\n\n        Args:\n            leaves_encoded (np.array): A tensor of one hot encoded leaves.\n\n            targets (np.array): An array of time targets for a specific time bin.\n\n        Returns:\n            lr_estimators (List): A list of fitted Logistic Regression models.\n                These models output calibrated survival probabilities for all times\n                in pre specified time bins.\n        \"\"\"\n\n        with Parallel(n_jobs=self.n_jobs) as parallel:\n            lr_estimators = parallel(\n                delayed(self._fit_one_lr)(leaves_encoded, targets[:, i])\n                for i in range(targets.shape[1])\n            )\n\n        return lr_estimators\n\n    def _predict_from_lr_list(self, lr_estimators, leaves_encoded, time_bins):\n        \"\"\"\n        Predicts survival probabilities from a list of multiple fitted\n        Logistic Regressions models. Encoded leaves are used as features.\n\n        Args:\n            lr_estimators (List): A list of fitted Logistic Regression models.\n            These models output calibrated survival probabilities for all times\n            in pre specified time bins.\n\n            leaves_encoded (np.array): A tensor of one hot encoded leaves.\n\n            time_bins (np.array): Specified time bins to split targets.\n\n        Returns:\n            preds (pd.DataFrame): A dataframe of estimated survival probabilities\n                for all times (columns), from the time_bins array, for all samples\n                (rows).\n        \"\"\"\n\n        with Parallel(n_jobs=self.n_jobs) as parallel:\n            preds = parallel(\n                delayed(m.predict_proba)(leaves_encoded) for m in lr_estimators\n            )\n\n        # organizing interval predictions from LRs\n        preds = np.array(preds)[:, :, 1].T\n        preds = pd.DataFrame(preds, columns=time_bins)\n\n        # converting these interval predictions\n        # to cumulative survival curve\n        return hazard_to_survival(preds)\n\n    def predict(self, X: pd.DataFrame, return_interval_probs: bool = False):\n        \"\"\"\n        Predicts survival probabilities using XGBoost + Logistic Regression pipeline.\n\n        Args:\n            X (pd.DataFrame): Dataframe of features to be used as input for the\n                XGBoost model.\n\n            return_interval_probs (Bool): Boolean indicating if interval probabilities\n                are to be returned. If False the cumulative survival is returned.\n                Default is False.\n\n        Returns:\n            pd.DataFrame: A dataframe of survival probabilities\n            for all times (columns), from a time_bins array, for all samples of X\n            (rows). If return_interval_probs is True, the interval probabilities are\n            returned instead of the cumulative survival probabilities.\n        \"\"\"\n\n        # getting leaves and extracting neighbors\n        leaves = self.feature_extractor.predict_leaves(X)\n\n        leaves_encoded = self.encoder.transform(leaves)\n\n        # predicting from logistic regression artifacts\n\n        preds_df = self._predict_from_lr_list(\n            self.lr_estimators_, leaves_encoded, self.time_bins\n        )\n\n        if return_interval_probs:\n            preds_df = calculate_interval_failures(preds_df)\n\n        return preds_df\n</code></pre>"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.__init__","title":"<code>__init__(self, xgb_params=None, lr_params={}, n_jobs=1, enable_categorical=False)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xgb_params</code> <code>Dict, None</code> <p>Parameters for XGBoost model. If None, will use XGBoost defaults and set objective as <code>survival:aft</code>. Check https://xgboost.readthedocs.io/en/latest/parameter.html for options.</p> <code>None</code> <code>lr_params</code> <code>Dict, None</code> <p>Parameters for LogisticRegression model. If None, will use scikit-learn default parameters.</p> <code>{}</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs used for parallel training of logistic regressions.</p> <code>1</code> <code>enable_categorical</code> <code>bool</code> <p>Enable categorical feature support on xgboost model</p> <code>False</code> Source code in <code>xgbse/_debiased_bce.py</code> <pre><code>def __init__(\n    self,\n    xgb_params: Optional[Dict[str, Any]] = None,\n    lr_params: Dict[str, Any] = {},\n    n_jobs: int = 1,\n    enable_categorical: bool = False,\n):\n    \"\"\"\n    Args:\n        xgb_params (Dict, None): Parameters for XGBoost model.\n            If None, will use XGBoost defaults and set objective as `survival:aft`.\n            Check &lt;https://xgboost.readthedocs.io/en/latest/parameter.html&gt; for options.\n\n        lr_params (Dict, None): Parameters for LogisticRegression model.\n            If None, will use scikit-learn default parameters.\n\n        n_jobs (int): Number of jobs used for parallel training of logistic regressions.\n\n        enable_categorical (bool): Enable categorical feature support on xgboost model\n    \"\"\"\n    super().__init__(xgb_params=xgb_params, enable_categorical=enable_categorical)\n    self.lr_params = lr_params\n    self.n_jobs = n_jobs\n</code></pre>"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.fit","title":"<code>fit(self, X, y, time_bins=None, validation_data=None, num_boost_round=10, early_stopping_rounds=None, verbose_eval=0, persist_train=False, index_id=None)</code>","text":"<p>Transform feature space by fitting a XGBoost model and returning its leafs. Leaves are transformed and considered as dummy variables to fit multiple logistic regression models to each evaluated time bin.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>[pd.DataFrame, np.array]</code> <p>Features to be used while fitting XGBoost model</p> required <code>y</code> <code>structured array(numpy.bool_, numpy.number</code> <p>Binary event indicator as first field, and time of event or time of censoring as second field.</p> required <code>time_bins</code> <code>np.array</code> <p>Specified time windows to use when making survival predictions</p> <code>None</code> <code>validation_data</code> <code>List[Tuple]</code> <p>Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping</p> <code>None</code> <code>num_boost_round</code> <code>Int</code> <p>Number of boosting iterations.</p> <code>10</code> <code>early_stopping_rounds</code> <code>Int</code> <p>Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation.</p> <code>None</code> <code>verbose_eval</code> <code>[Bool, Int]</code> <p>Level of verbosity. See xgboost.train documentation.</p> <code>0</code> <code>persist_train</code> <code>Bool</code> <p>Whether or not to persist training data to use explainability through prototypes</p> <code>False</code> <code>index_id</code> <code>pd.Index</code> <p>User defined index if intended to use explainability through prototypes</p> <code>None</code> <p>Returns:</p> Type Description <code>XGBSEDebiasedBCE</code> <p>Trained XGBSEDebiasedBCE instance</p> Source code in <code>xgbse/_debiased_bce.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    time_bins: Optional[Sequence] = None,\n    validation_data: Optional[List[Tuple[Any, Any]]] = None,\n    num_boost_round: int = 10,\n    early_stopping_rounds: Optional[int] = None,\n    verbose_eval: int = 0,\n    persist_train: bool = False,\n    index_id=None,\n):\n    \"\"\"\n    Transform feature space by fitting a XGBoost model and returning its leafs.\n    Leaves are transformed and considered as dummy variables to fit multiple\n    logistic regression models to each evaluated time bin.\n\n    Args:\n        X ([pd.DataFrame, np.array]): Features to be used while fitting XGBoost model\n\n        y (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n            and time of event or time of censoring as second field.\n\n        time_bins (np.array): Specified time windows to use when making survival predictions\n\n        validation_data (List[Tuple]): Validation data in the format of a list of tuples [(X, y)]\n            if user desires to use early stopping\n\n        num_boost_round (Int): Number of boosting iterations.\n\n        early_stopping_rounds (Int): Activates early stopping.\n            Validation metric needs to improve at least once\n            in every **early_stopping_rounds** round(s) to continue training.\n            See xgboost.train documentation.\n\n        verbose_eval ([Bool, Int]): Level of verbosity. See xgboost.train documentation.\n\n        persist_train (Bool): Whether or not to persist training data to use explainability\n            through prototypes\n\n        index_id (pd.Index): User defined index if intended to use explainability\n            through prototypes\n\n    Returns:\n        XGBSEDebiasedBCE: Trained XGBSEDebiasedBCE instance\n    \"\"\"\n    self.fit_feature_extractor(\n        X,\n        y,\n        time_bins=time_bins,\n        validation_data=validation_data,\n        num_boost_round=num_boost_round,\n        early_stopping_rounds=early_stopping_rounds,\n        verbose_eval=verbose_eval,\n    )\n\n    E_train, T_train = convert_y(y)\n    # predicting and encoding leaves\n    self.encoder = OneHotEncoder()\n    leaves = self.feature_extractor.predict_leaves(X)\n    leaves_encoded = self.encoder.fit_transform(leaves)\n\n    # convert targets for using with logistic regression\n    self.targets, self.time_bins = _build_multi_task_targets(\n        E_train, T_train, self.feature_extractor.time_bins\n    )\n\n    # fitting LR for several targets\n    self.lr_estimators_ = self._fit_all_lr(leaves_encoded, self.targets)\n\n    if persist_train:\n        self.persist_train = True\n        if index_id is None:\n            index_id = X.index.copy()\n        self.tree = BallTree(leaves, metric=\"hamming\")\n\n    self.index_id = index_id\n\n    return self\n</code></pre>"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.predict","title":"<code>predict(self, X, return_interval_probs=False)</code>","text":"<p>Predicts survival probabilities using XGBoost + Logistic Regression pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pd.DataFrame</code> <p>Dataframe of features to be used as input for the XGBoost model.</p> required <code>return_interval_probs</code> <code>Bool</code> <p>Boolean indicating if interval probabilities are to be returned. If False the cumulative survival is returned. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities.</p> Source code in <code>xgbse/_debiased_bce.py</code> <pre><code>def predict(self, X: pd.DataFrame, return_interval_probs: bool = False):\n    \"\"\"\n    Predicts survival probabilities using XGBoost + Logistic Regression pipeline.\n\n    Args:\n        X (pd.DataFrame): Dataframe of features to be used as input for the\n            XGBoost model.\n\n        return_interval_probs (Bool): Boolean indicating if interval probabilities\n            are to be returned. If False the cumulative survival is returned.\n            Default is False.\n\n    Returns:\n        pd.DataFrame: A dataframe of survival probabilities\n        for all times (columns), from a time_bins array, for all samples of X\n        (rows). If return_interval_probs is True, the interval probabilities are\n        returned instead of the cumulative survival probabilities.\n    \"\"\"\n\n    # getting leaves and extracting neighbors\n    leaves = self.feature_extractor.predict_leaves(X)\n\n    leaves_encoded = self.encoder.transform(leaves)\n\n    # predicting from logistic regression artifacts\n\n    preds_df = self._predict_from_lr_list(\n        self.lr_estimators_, leaves_encoded, self.time_bins\n    )\n\n    if return_interval_probs:\n        preds_df = calculate_interval_failures(preds_df)\n\n    return preds_df\n</code></pre>"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.set_fit_request","title":"<code>set_fit_request(self, *, early_stopping_rounds='$UNCHANGED$', index_id='$UNCHANGED$', num_boost_round='$UNCHANGED$', persist_train='$UNCHANGED$', time_bins='$UNCHANGED$', validation_data='$UNCHANGED$', verbose_eval='$UNCHANGED$')</code>","text":"<p>Request metadata passed to the <code>fit</code> method.</p> <p>Note that this method is only relevant if <code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>). Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing mechanism works.</p> <p>The options for each parameter are:</p> <ul> <li> <p><code>True</code>: metadata is requested, and passed to <code>fit</code> if provided. The request is ignored if metadata is not provided.</p> </li> <li> <p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>fit</code>.</p> </li> <li> <p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p> </li> <li> <p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p> </li> </ul> <p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the existing request. This allows you to change the request for some parameters and not others.</p> <p>.. versionadded:: 1.3</p> <p>.. note::     This method is only relevant if this estimator is used as a     sub-estimator of a meta-estimator, e.g. used inside a     :class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.set_fit_request--parameters","title":"Parameters","text":"<p>early_stopping_rounds : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>early_stopping_rounds</code> parameter in <code>fit</code>.</p> <p>index_id : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>index_id</code> parameter in <code>fit</code>.</p> <p>num_boost_round : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>num_boost_round</code> parameter in <code>fit</code>.</p> <p>persist_train : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>persist_train</code> parameter in <code>fit</code>.</p> <p>time_bins : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>time_bins</code> parameter in <code>fit</code>.</p> <p>validation_data : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>validation_data</code> parameter in <code>fit</code>.</p> <p>verbose_eval : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>verbose_eval</code> parameter in <code>fit</code>.</p>"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.set_fit_request--returns","title":"Returns","text":"<p>self : object     The updated object.</p> Source code in <code>xgbse/_debiased_bce.py</code> <pre><code>def func(*args, **kw):\n    \"\"\"Updates the request for provided parameters\n\n    This docstring is overwritten below.\n    See REQUESTER_DOC for expected functionality\n    \"\"\"\n    if not _routing_enabled():\n        raise RuntimeError(\n            \"This method is only available when metadata routing is enabled.\"\n            \" You can enable it using\"\n            \" sklearn.set_config(enable_metadata_routing=True).\"\n        )\n\n    if self.validate_keys and (set(kw) - set(self.keys)):\n        raise TypeError(\n            f\"Unexpected args: {set(kw) - set(self.keys)} in {self.name}. \"\n            f\"Accepted arguments are: {set(self.keys)}\"\n        )\n\n    # This makes it possible to use the decorated method as an unbound method,\n    # for instance when monkeypatching.\n    # https://github.com/scikit-learn/scikit-learn/issues/28632\n    if instance is None:\n        _instance = args[0]\n        args = args[1:]\n    else:\n        _instance = instance\n\n    # Replicating python's behavior when positional args are given other than\n    # `self`, and `self` is only allowed if this method is unbound.\n    if args:\n        raise TypeError(\n            f\"set_{self.name}_request() takes 0 positional argument but\"\n            f\" {len(args)} were given\"\n        )\n\n    requests = _instance._get_metadata_request()\n    method_metadata_request = getattr(requests, self.name)\n\n    for prop, alias in kw.items():\n        if alias is not UNCHANGED:\n            method_metadata_request.add_request(param=prop, alias=alias)\n    _instance._metadata_request = requests\n\n    return _instance\n</code></pre>"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.set_predict_request","title":"<code>set_predict_request(self, *, return_interval_probs='$UNCHANGED$')</code>","text":"<p>Request metadata passed to the <code>predict</code> method.</p> <p>Note that this method is only relevant if <code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>). Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing mechanism works.</p> <p>The options for each parameter are:</p> <ul> <li> <p><code>True</code>: metadata is requested, and passed to <code>predict</code> if provided. The request is ignored if metadata is not provided.</p> </li> <li> <p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>predict</code>.</p> </li> <li> <p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p> </li> <li> <p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p> </li> </ul> <p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the existing request. This allows you to change the request for some parameters and not others.</p> <p>.. versionadded:: 1.3</p> <p>.. note::     This method is only relevant if this estimator is used as a     sub-estimator of a meta-estimator, e.g. used inside a     :class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.set_predict_request--parameters","title":"Parameters","text":"<p>return_interval_probs : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>return_interval_probs</code> parameter in <code>predict</code>.</p>"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.set_predict_request--returns","title":"Returns","text":"<p>self : object     The updated object.</p> Source code in <code>xgbse/_debiased_bce.py</code> <pre><code>def func(*args, **kw):\n    \"\"\"Updates the request for provided parameters\n\n    This docstring is overwritten below.\n    See REQUESTER_DOC for expected functionality\n    \"\"\"\n    if not _routing_enabled():\n        raise RuntimeError(\n            \"This method is only available when metadata routing is enabled.\"\n            \" You can enable it using\"\n            \" sklearn.set_config(enable_metadata_routing=True).\"\n        )\n\n    if self.validate_keys and (set(kw) - set(self.keys)):\n        raise TypeError(\n            f\"Unexpected args: {set(kw) - set(self.keys)} in {self.name}. \"\n            f\"Accepted arguments are: {set(self.keys)}\"\n        )\n\n    # This makes it possible to use the decorated method as an unbound method,\n    # for instance when monkeypatching.\n    # https://github.com/scikit-learn/scikit-learn/issues/28632\n    if instance is None:\n        _instance = args[0]\n        args = args[1:]\n    else:\n        _instance = instance\n\n    # Replicating python's behavior when positional args are given other than\n    # `self`, and `self` is only allowed if this method is unbound.\n    if args:\n        raise TypeError(\n            f\"set_{self.name}_request() takes 0 positional argument but\"\n            f\" {len(args)} were given\"\n        )\n\n    requests = _instance._get_metadata_request()\n    method_metadata_request = getattr(requests, self.name)\n\n    for prop, alias in kw.items():\n        if alias is not UNCHANGED:\n            method_metadata_request.add_request(param=prop, alias=alias)\n    _instance._metadata_request = requests\n\n    return _instance\n</code></pre>"},{"location":"modules/converters.html","title":"xgbse.converters.convert_to_structured","text":"<p>Converts data in time (T) and event (E) format to a structured numpy array. Provides common interface to other libraries such as sksurv and sklearn.</p> <p>Parameters:</p> Name Type Description Default <code>T</code> <code>np.array</code> <p>Array of times</p> required <code>E</code> <code>np.array</code> <p>Array of events</p> required <p>Returns:</p> Type Description <code>np.array</code> <p>Structured array containing the boolean event indicator     as first field, and time of event or time of censoring as second field</p> Source code in <code>xgbse/converters.py</code> <pre><code>def convert_to_structured(T, E):\n    \"\"\"\n    Converts data in time (T) and event (E) format to a structured numpy array.\n    Provides common interface to other libraries such as sksurv and sklearn.\n\n    Args:\n        T (np.array): Array of times\n        E (np.array): Array of events\n\n    Returns:\n        np.array: Structured array containing the boolean event indicator\n            as first field, and time of event or time of censoring as second field\n    \"\"\"\n    # dtypes for conversion\n    default_dtypes = {\"names\": (\"c1\", \"c2\"), \"formats\": (\"bool\", \"f8\")}\n\n    # concat of events and times\n    concat = list(zip(E.values, T.values))\n\n    # return structured array\n    return np.array(concat, dtype=default_dtypes)\n</code></pre>"},{"location":"modules/converters.html#xgbseconvertersconvert_y","title":"xgbse.converters.convert_y","text":"<p>Convert structured array y into an array of event indicators (E) and time of events (T).</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>structured array(numpy.bool_, numpy.number</code> <p>Binary event indicator as first field, and time of event or time of censoring as second field.</p> required <p>Returns:</p> Type Description <code>T ([np.array, pd.Series])</code> <p>Time of events E ([np.array, pd.Series]): Binary event indicator</p> Source code in <code>xgbse/converters.py</code> <pre><code>def convert_y(y):\n    \"\"\"\n    Convert structured array y into an array of\n    event indicators (E) and time of events (T).\n\n    Args:\n        y (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n            and time of event or time of censoring as second field.\n\n    Returns:\n        T ([np.array, pd.Series]): Time of events\n        E ([np.array, pd.Series]): Binary event indicator\n    \"\"\"\n    event_field, time_field = y.dtype.names\n    return y[event_field], y[time_field]\n</code></pre>"},{"location":"modules/extrapolation.html","title":"xgbse.extrapolation","text":""},{"location":"modules/extrapolation.html#xgbse.extrapolation.extrapolate_constant_risk","title":"<code>extrapolate_constant_risk(survival, final_time, intervals, lags=-1)</code>","text":"<p>Extrapolate a survival curve assuming constant risk.</p> <p>Parameters:</p> Name Type Description Default <code>survival</code> <code>pd.DataFrame</code> <p>A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows).</p> required <code>final_time</code> <code>Float</code> <p>Final time for extrapolation</p> required <code>intervals</code> <code>Int</code> <p>Time in each interval between last time in survival dataframe and final time</p> required <code>lags</code> <code>Int</code> <p>Lags to compute constant risk. if negative, will use the last \"lags\" values if positive, will remove the first \"lags\" values if 0, will use all values</p> <code>-1</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Survival dataset with appended extrapolated windows</p> Source code in <code>xgbse/extrapolation.py</code> <pre><code>def extrapolate_constant_risk(survival, final_time, intervals, lags=-1):\n    \"\"\"\n    Extrapolate a survival curve assuming constant risk.\n\n    Args:\n        survival (pd.DataFrame): A dataframe of survival probabilities\n            for all times (columns), from a time_bins array, for all samples of X (rows).\n\n        final_time (Float): Final time for extrapolation\n\n        intervals (Int): Time in each interval between last time in survival dataframe and final time\n\n        lags (Int): Lags to compute constant risk.\n            if negative, will use the last \"lags\" values\n            if positive, will remove the first \"lags\" values\n            if 0, will use all values\n\n    Returns:\n        pd.DataFrame: Survival dataset with appended extrapolated windows\n    \"\"\"\n\n    last_time = survival.columns[-1]\n    # creating windows for extrapolation\n    # here we sum intervals in times to exclude the last time, that already is in surv dataframe and\n    #  to include final time in resulting dataframe\n    extrap_windows = np.arange(last_time + intervals, final_time + intervals, intervals)\n\n    # calculating conditionals and hazard at each time window\n    hazards = _get_conditional_probs_from_survival(survival)\n\n    # calculating avg hazard for desired lags\n    constant_haz = hazards.values[:, lags:].mean(axis=1).reshape(-1, 1)\n\n    # repeat hazard for n_windows required\n    constant_haz = np.tile(constant_haz, len(extrap_windows))\n\n    constant_haz = pd.DataFrame(constant_haz, columns=extrap_windows)\n\n    hazards = pd.concat([hazards, constant_haz], axis=1)\n\n    return hazard_to_survival(hazards)\n</code></pre>"},{"location":"modules/kaplan_neighs.html","title":"xgbse._kaplan_neighbors.XGBSEKaplanNeighbors","text":"<p>Convert xgboost into a nearest neighbor model, where we use hamming distance to define similar elements as the ones that co-ocurred the most at the ensemble terminal nodes.</p> <p>Then, at each neighbor-set compute survival estimates with the Kaplan-Meier estimator.</p> <p>Note</p> <ul> <li> <p>We recommend using dart as the booster to prevent any tree to dominate variance in the ensemble and break the leaf co-ocurrence similarity logic.</p> </li> <li> <p>This method can be very expensive at scales of hundreds of thousands of samples, due to the nearest neighbor search, both on training (construction of search index) and scoring (actual search).</p> </li> </ul> <p>Read more in How XGBSE works.</p> Source code in <code>xgbse/_kaplan_neighbors.py</code> <pre><code>class XGBSEKaplanNeighbors(XGBSEBaseEstimator):\n    \"\"\"\n    Convert xgboost into a nearest neighbor model, where we use hamming distance to define\n    similar elements as the ones that co-ocurred the most at the ensemble terminal nodes.\n\n    Then, at each neighbor-set compute survival estimates with the Kaplan-Meier estimator.\n\n    !!! Note\n        * We recommend using dart as the booster to prevent any tree\n        to dominate variance in the ensemble and break the leaf co-ocurrence similarity logic.\n\n        * This method can be very expensive at scales of hundreds of thousands of samples,\n        due to the nearest neighbor search, both on training (construction of search index) and scoring (actual search).\n\n    Read more in [How XGBSE works](https://loft-br.github.io/xgboost-survival-embeddings/how_xgbse_works.html).\n    \"\"\"\n\n    def __init__(\n        self,\n        xgb_params: Optional[Dict[str, Any]] = None,\n        n_neighbors: int = 30,\n        radius: Optional[float] = None,\n        enable_categorical: bool = False,\n    ):\n        \"\"\"\n        Args:\n            xgb_params (Dict, None): Parameters for XGBoost model.\n                If None, will use XGBoost defaults and set objective as `survival:aft`.\n                Check &lt;https://xgboost.readthedocs.io/en/latest/parameter.html&gt; for options.\n\n            n_neighbors (Int): Number of neighbors for computing KM estimates\n\n            radius (Float): If set, uses a radius around the point for neighbors search\n\n            enable_categorical (bool): Enable categorical feature support on xgboost model\n        \"\"\"\n\n        super().__init__(xgb_params=xgb_params, enable_categorical=enable_categorical)\n        self.n_neighbors = n_neighbors\n        self.radius = radius\n        self.index_id = None\n\n    def fit(\n        self,\n        X,\n        y,\n        time_bins: Optional[Sequence] = None,\n        validation_data: Optional[List[Tuple[Any, Any]]] = None,\n        num_boost_round: int = 10,\n        early_stopping_rounds: Optional[int] = None,\n        verbose_eval: int = 0,\n        persist_train: bool = False,\n        index_id=None,\n    ):\n        \"\"\"\n        Transform feature space by fitting a XGBoost model and outputting its leaf indices.\n        Build search index in the new space to allow nearest neighbor queries at scoring time.\n\n        Args:\n            X ([pd.DataFrame, np.array]): Features to be used while fitting XGBoost model\n\n            y (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n                and time of event or time of censoring as second field.\n\n            time_bins (np.array): Specified time windows to use when making survival predictions\n\n            validation_data (List[Tuple]): Validation data in the format of a list of tuples [(X, y)]\n                if user desires to use early stopping\n\n            num_boost_round (Int): Number of boosting iterations.\n\n            early_stopping_rounds (Int): Activates early stopping.\n                Validation metric needs to improve at least once\n                in every **early_stopping_rounds** round(s) to continue training.\n                See xgboost.train documentation.\n\n            verbose_eval ([Bool, Int]): Level of verbosity. See xgboost.train documentation.\n\n            persist_train (Bool): Whether or not to persist training data to use explainability\n                through prototypes\n\n            index_id (pd.Index): User defined index if intended to use explainability\n                through prototypes\n\n\n        Returns:\n            XGBSEKaplanNeighbors: Fitted instance of XGBSEKaplanNeighbors\n        \"\"\"\n\n        self.fit_feature_extractor(\n            X,\n            y,\n            time_bins=time_bins,\n            validation_data=validation_data,\n            num_boost_round=num_boost_round,\n            early_stopping_rounds=early_stopping_rounds,\n            verbose_eval=verbose_eval,\n        )\n\n        self.E_train, self.T_train = convert_y(y)\n\n        # creating nearest neighbor index\n        leaves = self.feature_extractor.predict_leaves(X)\n\n        self.tree = BallTree(leaves, metric=\"hamming\", leaf_size=40)\n\n        if persist_train:\n            self.persist_train = True\n            if index_id is None:\n                index_id = X.index.copy()\n        self.index_id = index_id\n\n        return self\n\n    def predict(\n        self,\n        X,\n        time_bins=None,\n        return_ci=False,\n        ci_width=0.683,\n        return_interval_probs=False,\n    ):\n        \"\"\"\n        Make queries to nearest neighbor search index build on the transformed XGBoost space.\n        Compute a Kaplan-Meier estimator for each neighbor-set. Predict the KM estimators.\n\n        Args:\n            X (pd.DataFrame): Dataframe with samples to generate predictions\n\n            time_bins (np.array): Specified time windows to use when making survival predictions\n\n            return_ci (Bool): Whether to return confidence intervals via the Exponential Greenwood formula\n\n            ci_width (Float): Width of confidence interval\n\n            return_interval_probs (Bool): Boolean indicating if interval probabilities are\n                supposed to be returned. If False the cumulative survival is returned.\n\n\n        Returns:\n            (pd.DataFrame): A dataframe of survival probabilities\n            for all times (columns), from a time_bins array, for all samples of X\n            (rows). If return_interval_probs is True, the interval probabilities are returned\n            instead of the cumulative survival probabilities.\n\n            upper_ci (np.array): Upper confidence interval for the survival\n            probability values\n\n            lower_ci (np.array): Lower confidence interval for the survival\n            probability values\n        \"\"\"\n\n        leaves = self.feature_extractor.predict_leaves(X)\n\n        if self.radius:\n            assert self.radius &gt;= 0, \"Radius must be greater than 0\"\n\n            neighs, _ = self.tree.query_radius(\n                leaves, r=self.radius, return_distance=True\n            )\n\n            number_of_neighbors = np.array([len(neigh) for neigh in neighs])\n\n            if np.argwhere(number_of_neighbors == 1).shape[0] &gt; 0:\n                # If there is at least one sample without neighbors apart from itself\n                # a warning is raised suggesting a radius increase\n                warnings.warn(\n                    \"Warning: Some samples don't have neighbors apart from itself. Increase the radius\",\n                    RuntimeWarning,\n                )\n        else:\n            _, neighs = self.tree.query(leaves, k=self.n_neighbors)\n\n        # gathering times and events/censors for neighbor sets\n        T_neighs = self.T_train[neighs]\n        E_neighs = self.E_train[neighs]\n\n        # vectorized (very fast!) implementation of Kaplan Meier curves\n        if time_bins is None:\n            time_bins = self.time_bins\n\n        # calculating z-score from width\n        z = st.norm.ppf(0.5 + ci_width / 2)\n\n        preds_df, upper_ci, lower_ci = calculate_kaplan_vectorized(\n            T_neighs, E_neighs, time_bins, z\n        )\n\n        if return_ci and return_interval_probs:\n            raise ValueError(\n                \"Confidence intervals for interval probabilities is not supported. Choose between return_ci and return_interval_probs.\"\n            )\n\n        if return_interval_probs:\n            preds_df = calculate_interval_failures(preds_df)\n            return preds_df\n\n        if return_ci:\n            return preds_df, upper_ci, lower_ci\n\n        return preds_df\n</code></pre>"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.__init__","title":"<code>__init__(self, xgb_params=None, n_neighbors=30, radius=None, enable_categorical=False)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xgb_params</code> <code>Dict, None</code> <p>Parameters for XGBoost model. If None, will use XGBoost defaults and set objective as <code>survival:aft</code>. Check https://xgboost.readthedocs.io/en/latest/parameter.html for options.</p> <code>None</code> <code>n_neighbors</code> <code>Int</code> <p>Number of neighbors for computing KM estimates</p> <code>30</code> <code>radius</code> <code>Float</code> <p>If set, uses a radius around the point for neighbors search</p> <code>None</code> <code>enable_categorical</code> <code>bool</code> <p>Enable categorical feature support on xgboost model</p> <code>False</code> Source code in <code>xgbse/_kaplan_neighbors.py</code> <pre><code>def __init__(\n    self,\n    xgb_params: Optional[Dict[str, Any]] = None,\n    n_neighbors: int = 30,\n    radius: Optional[float] = None,\n    enable_categorical: bool = False,\n):\n    \"\"\"\n    Args:\n        xgb_params (Dict, None): Parameters for XGBoost model.\n            If None, will use XGBoost defaults and set objective as `survival:aft`.\n            Check &lt;https://xgboost.readthedocs.io/en/latest/parameter.html&gt; for options.\n\n        n_neighbors (Int): Number of neighbors for computing KM estimates\n\n        radius (Float): If set, uses a radius around the point for neighbors search\n\n        enable_categorical (bool): Enable categorical feature support on xgboost model\n    \"\"\"\n\n    super().__init__(xgb_params=xgb_params, enable_categorical=enable_categorical)\n    self.n_neighbors = n_neighbors\n    self.radius = radius\n    self.index_id = None\n</code></pre>"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.fit","title":"<code>fit(self, X, y, time_bins=None, validation_data=None, num_boost_round=10, early_stopping_rounds=None, verbose_eval=0, persist_train=False, index_id=None)</code>","text":"<p>Transform feature space by fitting a XGBoost model and outputting its leaf indices. Build search index in the new space to allow nearest neighbor queries at scoring time.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>[pd.DataFrame, np.array]</code> <p>Features to be used while fitting XGBoost model</p> required <code>y</code> <code>structured array(numpy.bool_, numpy.number</code> <p>Binary event indicator as first field, and time of event or time of censoring as second field.</p> required <code>time_bins</code> <code>np.array</code> <p>Specified time windows to use when making survival predictions</p> <code>None</code> <code>validation_data</code> <code>List[Tuple]</code> <p>Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping</p> <code>None</code> <code>num_boost_round</code> <code>Int</code> <p>Number of boosting iterations.</p> <code>10</code> <code>early_stopping_rounds</code> <code>Int</code> <p>Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation.</p> <code>None</code> <code>verbose_eval</code> <code>[Bool, Int]</code> <p>Level of verbosity. See xgboost.train documentation.</p> <code>0</code> <code>persist_train</code> <code>Bool</code> <p>Whether or not to persist training data to use explainability through prototypes</p> <code>False</code> <code>index_id</code> <code>pd.Index</code> <p>User defined index if intended to use explainability through prototypes</p> <code>None</code> <p>Returns:</p> Type Description <code>XGBSEKaplanNeighbors</code> <p>Fitted instance of XGBSEKaplanNeighbors</p> Source code in <code>xgbse/_kaplan_neighbors.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    time_bins: Optional[Sequence] = None,\n    validation_data: Optional[List[Tuple[Any, Any]]] = None,\n    num_boost_round: int = 10,\n    early_stopping_rounds: Optional[int] = None,\n    verbose_eval: int = 0,\n    persist_train: bool = False,\n    index_id=None,\n):\n    \"\"\"\n    Transform feature space by fitting a XGBoost model and outputting its leaf indices.\n    Build search index in the new space to allow nearest neighbor queries at scoring time.\n\n    Args:\n        X ([pd.DataFrame, np.array]): Features to be used while fitting XGBoost model\n\n        y (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n            and time of event or time of censoring as second field.\n\n        time_bins (np.array): Specified time windows to use when making survival predictions\n\n        validation_data (List[Tuple]): Validation data in the format of a list of tuples [(X, y)]\n            if user desires to use early stopping\n\n        num_boost_round (Int): Number of boosting iterations.\n\n        early_stopping_rounds (Int): Activates early stopping.\n            Validation metric needs to improve at least once\n            in every **early_stopping_rounds** round(s) to continue training.\n            See xgboost.train documentation.\n\n        verbose_eval ([Bool, Int]): Level of verbosity. See xgboost.train documentation.\n\n        persist_train (Bool): Whether or not to persist training data to use explainability\n            through prototypes\n\n        index_id (pd.Index): User defined index if intended to use explainability\n            through prototypes\n\n\n    Returns:\n        XGBSEKaplanNeighbors: Fitted instance of XGBSEKaplanNeighbors\n    \"\"\"\n\n    self.fit_feature_extractor(\n        X,\n        y,\n        time_bins=time_bins,\n        validation_data=validation_data,\n        num_boost_round=num_boost_round,\n        early_stopping_rounds=early_stopping_rounds,\n        verbose_eval=verbose_eval,\n    )\n\n    self.E_train, self.T_train = convert_y(y)\n\n    # creating nearest neighbor index\n    leaves = self.feature_extractor.predict_leaves(X)\n\n    self.tree = BallTree(leaves, metric=\"hamming\", leaf_size=40)\n\n    if persist_train:\n        self.persist_train = True\n        if index_id is None:\n            index_id = X.index.copy()\n    self.index_id = index_id\n\n    return self\n</code></pre>"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.predict","title":"<code>predict(self, X, time_bins=None, return_ci=False, ci_width=0.683, return_interval_probs=False)</code>","text":"<p>Make queries to nearest neighbor search index build on the transformed XGBoost space. Compute a Kaplan-Meier estimator for each neighbor-set. Predict the KM estimators.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pd.DataFrame</code> <p>Dataframe with samples to generate predictions</p> required <code>time_bins</code> <code>np.array</code> <p>Specified time windows to use when making survival predictions</p> <code>None</code> <code>return_ci</code> <code>Bool</code> <p>Whether to return confidence intervals via the Exponential Greenwood formula</p> <code>False</code> <code>ci_width</code> <code>Float</code> <p>Width of confidence interval</p> <code>0.683</code> <code>return_interval_probs</code> <code>Bool</code> <p>Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>(pd.DataFrame)</code> <p>A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities.</p> <p>upper_ci (np.array): Upper confidence interval for the survival probability values</p> <p>lower_ci (np.array): Lower confidence interval for the survival probability values</p> Source code in <code>xgbse/_kaplan_neighbors.py</code> <pre><code>def predict(\n    self,\n    X,\n    time_bins=None,\n    return_ci=False,\n    ci_width=0.683,\n    return_interval_probs=False,\n):\n    \"\"\"\n    Make queries to nearest neighbor search index build on the transformed XGBoost space.\n    Compute a Kaplan-Meier estimator for each neighbor-set. Predict the KM estimators.\n\n    Args:\n        X (pd.DataFrame): Dataframe with samples to generate predictions\n\n        time_bins (np.array): Specified time windows to use when making survival predictions\n\n        return_ci (Bool): Whether to return confidence intervals via the Exponential Greenwood formula\n\n        ci_width (Float): Width of confidence interval\n\n        return_interval_probs (Bool): Boolean indicating if interval probabilities are\n            supposed to be returned. If False the cumulative survival is returned.\n\n\n    Returns:\n        (pd.DataFrame): A dataframe of survival probabilities\n        for all times (columns), from a time_bins array, for all samples of X\n        (rows). If return_interval_probs is True, the interval probabilities are returned\n        instead of the cumulative survival probabilities.\n\n        upper_ci (np.array): Upper confidence interval for the survival\n        probability values\n\n        lower_ci (np.array): Lower confidence interval for the survival\n        probability values\n    \"\"\"\n\n    leaves = self.feature_extractor.predict_leaves(X)\n\n    if self.radius:\n        assert self.radius &gt;= 0, \"Radius must be greater than 0\"\n\n        neighs, _ = self.tree.query_radius(\n            leaves, r=self.radius, return_distance=True\n        )\n\n        number_of_neighbors = np.array([len(neigh) for neigh in neighs])\n\n        if np.argwhere(number_of_neighbors == 1).shape[0] &gt; 0:\n            # If there is at least one sample without neighbors apart from itself\n            # a warning is raised suggesting a radius increase\n            warnings.warn(\n                \"Warning: Some samples don't have neighbors apart from itself. Increase the radius\",\n                RuntimeWarning,\n            )\n    else:\n        _, neighs = self.tree.query(leaves, k=self.n_neighbors)\n\n    # gathering times and events/censors for neighbor sets\n    T_neighs = self.T_train[neighs]\n    E_neighs = self.E_train[neighs]\n\n    # vectorized (very fast!) implementation of Kaplan Meier curves\n    if time_bins is None:\n        time_bins = self.time_bins\n\n    # calculating z-score from width\n    z = st.norm.ppf(0.5 + ci_width / 2)\n\n    preds_df, upper_ci, lower_ci = calculate_kaplan_vectorized(\n        T_neighs, E_neighs, time_bins, z\n    )\n\n    if return_ci and return_interval_probs:\n        raise ValueError(\n            \"Confidence intervals for interval probabilities is not supported. Choose between return_ci and return_interval_probs.\"\n        )\n\n    if return_interval_probs:\n        preds_df = calculate_interval_failures(preds_df)\n        return preds_df\n\n    if return_ci:\n        return preds_df, upper_ci, lower_ci\n\n    return preds_df\n</code></pre>"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.set_fit_request","title":"<code>set_fit_request(self, *, early_stopping_rounds='$UNCHANGED$', index_id='$UNCHANGED$', num_boost_round='$UNCHANGED$', persist_train='$UNCHANGED$', time_bins='$UNCHANGED$', validation_data='$UNCHANGED$', verbose_eval='$UNCHANGED$')</code>","text":"<p>Request metadata passed to the <code>fit</code> method.</p> <p>Note that this method is only relevant if <code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>). Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing mechanism works.</p> <p>The options for each parameter are:</p> <ul> <li> <p><code>True</code>: metadata is requested, and passed to <code>fit</code> if provided. The request is ignored if metadata is not provided.</p> </li> <li> <p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>fit</code>.</p> </li> <li> <p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p> </li> <li> <p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p> </li> </ul> <p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the existing request. This allows you to change the request for some parameters and not others.</p> <p>.. versionadded:: 1.3</p> <p>.. note::     This method is only relevant if this estimator is used as a     sub-estimator of a meta-estimator, e.g. used inside a     :class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.set_fit_request--parameters","title":"Parameters","text":"<p>early_stopping_rounds : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>early_stopping_rounds</code> parameter in <code>fit</code>.</p> <p>index_id : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>index_id</code> parameter in <code>fit</code>.</p> <p>num_boost_round : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>num_boost_round</code> parameter in <code>fit</code>.</p> <p>persist_train : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>persist_train</code> parameter in <code>fit</code>.</p> <p>time_bins : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>time_bins</code> parameter in <code>fit</code>.</p> <p>validation_data : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>validation_data</code> parameter in <code>fit</code>.</p> <p>verbose_eval : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>verbose_eval</code> parameter in <code>fit</code>.</p>"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.set_fit_request--returns","title":"Returns","text":"<p>self : object     The updated object.</p> Source code in <code>xgbse/_kaplan_neighbors.py</code> <pre><code>def func(*args, **kw):\n    \"\"\"Updates the request for provided parameters\n\n    This docstring is overwritten below.\n    See REQUESTER_DOC for expected functionality\n    \"\"\"\n    if not _routing_enabled():\n        raise RuntimeError(\n            \"This method is only available when metadata routing is enabled.\"\n            \" You can enable it using\"\n            \" sklearn.set_config(enable_metadata_routing=True).\"\n        )\n\n    if self.validate_keys and (set(kw) - set(self.keys)):\n        raise TypeError(\n            f\"Unexpected args: {set(kw) - set(self.keys)} in {self.name}. \"\n            f\"Accepted arguments are: {set(self.keys)}\"\n        )\n\n    # This makes it possible to use the decorated method as an unbound method,\n    # for instance when monkeypatching.\n    # https://github.com/scikit-learn/scikit-learn/issues/28632\n    if instance is None:\n        _instance = args[0]\n        args = args[1:]\n    else:\n        _instance = instance\n\n    # Replicating python's behavior when positional args are given other than\n    # `self`, and `self` is only allowed if this method is unbound.\n    if args:\n        raise TypeError(\n            f\"set_{self.name}_request() takes 0 positional argument but\"\n            f\" {len(args)} were given\"\n        )\n\n    requests = _instance._get_metadata_request()\n    method_metadata_request = getattr(requests, self.name)\n\n    for prop, alias in kw.items():\n        if alias is not UNCHANGED:\n            method_metadata_request.add_request(param=prop, alias=alias)\n    _instance._metadata_request = requests\n\n    return _instance\n</code></pre>"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.set_predict_request","title":"<code>set_predict_request(self, *, ci_width='$UNCHANGED$', return_ci='$UNCHANGED$', return_interval_probs='$UNCHANGED$', time_bins='$UNCHANGED$')</code>","text":"<p>Request metadata passed to the <code>predict</code> method.</p> <p>Note that this method is only relevant if <code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>). Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing mechanism works.</p> <p>The options for each parameter are:</p> <ul> <li> <p><code>True</code>: metadata is requested, and passed to <code>predict</code> if provided. The request is ignored if metadata is not provided.</p> </li> <li> <p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>predict</code>.</p> </li> <li> <p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p> </li> <li> <p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p> </li> </ul> <p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the existing request. This allows you to change the request for some parameters and not others.</p> <p>.. versionadded:: 1.3</p> <p>.. note::     This method is only relevant if this estimator is used as a     sub-estimator of a meta-estimator, e.g. used inside a     :class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.set_predict_request--parameters","title":"Parameters","text":"<p>ci_width : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>ci_width</code> parameter in <code>predict</code>.</p> <p>return_ci : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>return_ci</code> parameter in <code>predict</code>.</p> <p>return_interval_probs : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>return_interval_probs</code> parameter in <code>predict</code>.</p> <p>time_bins : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>time_bins</code> parameter in <code>predict</code>.</p>"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.set_predict_request--returns","title":"Returns","text":"<p>self : object     The updated object.</p> Source code in <code>xgbse/_kaplan_neighbors.py</code> <pre><code>def func(*args, **kw):\n    \"\"\"Updates the request for provided parameters\n\n    This docstring is overwritten below.\n    See REQUESTER_DOC for expected functionality\n    \"\"\"\n    if not _routing_enabled():\n        raise RuntimeError(\n            \"This method is only available when metadata routing is enabled.\"\n            \" You can enable it using\"\n            \" sklearn.set_config(enable_metadata_routing=True).\"\n        )\n\n    if self.validate_keys and (set(kw) - set(self.keys)):\n        raise TypeError(\n            f\"Unexpected args: {set(kw) - set(self.keys)} in {self.name}. \"\n            f\"Accepted arguments are: {set(self.keys)}\"\n        )\n\n    # This makes it possible to use the decorated method as an unbound method,\n    # for instance when monkeypatching.\n    # https://github.com/scikit-learn/scikit-learn/issues/28632\n    if instance is None:\n        _instance = args[0]\n        args = args[1:]\n    else:\n        _instance = instance\n\n    # Replicating python's behavior when positional args are given other than\n    # `self`, and `self` is only allowed if this method is unbound.\n    if args:\n        raise TypeError(\n            f\"set_{self.name}_request() takes 0 positional argument but\"\n            f\" {len(args)} were given\"\n        )\n\n    requests = _instance._get_metadata_request()\n    method_metadata_request = getattr(requests, self.name)\n\n    for prop, alias in kw.items():\n        if alias is not UNCHANGED:\n            method_metadata_request.add_request(param=prop, alias=alias)\n    _instance._metadata_request = requests\n\n    return _instance\n</code></pre>"},{"location":"modules/kaplan_tree.html","title":"xgbse._kaplan_neighbors.XGBSEKaplanTree","text":"<p>Single tree implementation as a simplification to <code>XGBSEKaplanNeighbors</code>. Instead of doing nearest neighbor searches, fits a single tree via <code>xgboost</code> and calculates KM curves at each of its leaves.</p> <p>Note</p> <ul> <li>It is by far the most efficient implementation, able to scale to millions of examples easily. At fit time, the tree is built and all KM curves are pre-calculated, so that at scoring time a simple query will suffice to get the model's estimates.</li> </ul> <p>Read more in How XGBSE works.</p> Source code in <code>xgbse/_kaplan_neighbors.py</code> <pre><code>class XGBSEKaplanTree(XGBSEBaseEstimator):\n    \"\"\"\n    Single tree implementation as a simplification to `XGBSEKaplanNeighbors`.\n    Instead of doing nearest neighbor searches, fits a single tree via `xgboost`\n    and calculates KM curves at each of its leaves.\n\n    !!! Note\n        * It is by far the most efficient implementation, able to scale to millions of examples easily.\n        At fit time, the tree is built and all KM curves are pre-calculated,\n        so that at scoring time a simple query will suffice to get the model's estimates.\n\n    Read more in [How XGBSE works](https://loft-br.github.io/xgboost-survival-embeddings/how_xgbse_works.html).\n    \"\"\"\n\n    def __init__(\n        self,\n        xgb_params: Optional[Dict[str, Any]] = None,\n        enable_categorical: bool = False,\n    ):\n        \"\"\"\n        Args:\n            xgb_params (Dict): Parameters for XGBoost model.\n                If not passed, the following default parameters will be used:\n\n                ```\n                DEFAULT_PARAMS_TREE = {\n                    \"objective\": \"survival:cox\",\n                    \"eval_metric\": \"cox-nloglik\",\n                    \"tree_method\": \"hist\",\n                    \"max_depth\": 100,\n                    \"booster\": \"dart\",\n                    \"subsample\": 1.0,\n                    \"min_child_weight\": 30,\n                    \"colsample_bynode\": 1.0,\n                }\n                ```\n\n                Check &lt;https://xgboost.readthedocs.io/en/latest/parameter.html&gt; for more options.\n        \"\"\"\n        if xgb_params is None:\n            xgb_params = DEFAULT_PARAMS_TREE\n\n        super().__init__(xgb_params=xgb_params, enable_categorical=enable_categorical)\n        self.index_id = None\n\n    def fit(\n        self,\n        X,\n        y,\n        persist_train: bool = True,\n        index_id=None,\n        time_bins: Optional[Sequence] = None,\n        ci_width: float = 0.683,\n    ):\n        \"\"\"\n        Fit a single decision tree using xgboost. For each leaf in the tree,\n        build a Kaplan-Meier estimator.\n\n        !!! Note\n            * Differently from `XGBSEKaplanNeighbors`, in `XGBSEKaplanTree`,\n            the width of the confidence interval (`ci_width`)\n            must be specified at fit time.\n\n        Args:\n\n            X ([pd.DataFrame, np.array]): Design matrix to fit XGBoost model\n\n            y (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n                and time of event or time of censoring as second field.\n\n            persist_train (Bool): Whether or not to persist training data to use explainability\n                through prototypes\n\n            index_id (pd.Index): User defined index if intended to use explainability\n                through prototypes\n\n            time_bins (np.array): Specified time windows to use when making survival predictions\n\n            ci_width (Float): Width of confidence interval\n\n        Returns:\n            XGBSEKaplanTree: Trained instance of XGBSEKaplanTree\n        \"\"\"\n\n        self.feature_extractor.fit(\n            X,\n            y,\n            time_bins=time_bins,\n            num_boost_round=1,\n        )\n        self.feature_importances_ = self.feature_extractor.feature_importances_\n\n        E_train, T_train = convert_y(y)\n\n        self.time_bins = self.feature_extractor.time_bins\n        # getting leaves\n        leaves = self.feature_extractor.predict_leaves(X)\n\n        # organizing elements per leaf\n        leaf_neighs = (\n            pd.DataFrame({\"leaf\": leaves})\n            .groupby(\"leaf\")\n            .apply(lambda x: list(x.index))\n        )\n\n        # getting T and E for each leaf\n        T_leaves = _align_leaf_target(leaf_neighs, T_train)\n        E_leaves = _align_leaf_target(leaf_neighs, E_train)\n\n        # calculating z-score from width\n        z = st.norm.ppf(0.5 + ci_width / 2)\n\n        # vectorized (very fast!) implementation of Kaplan Meier curves\n        (\n            self._train_survival,\n            self._train_upper_ci,\n            self._train_lower_ci,\n        ) = calculate_kaplan_vectorized(T_leaves, E_leaves, self.time_bins, z)\n\n        # adding leaf indexes\n        self._train_survival = self._train_survival.set_index(leaf_neighs.index)\n        self._train_upper_ci = self._train_upper_ci.set_index(leaf_neighs.index)\n        self._train_lower_ci = self._train_lower_ci.set_index(leaf_neighs.index)\n\n        if persist_train:\n            self.persist_train = True\n            if index_id is None:\n                index_id = X.index.copy()\n            self.tree = BallTree(leaves.reshape(-1, 1), metric=\"hamming\", leaf_size=40)\n        self.index_id = index_id\n\n        return self\n\n    def predict(self, X, return_ci=False, return_interval_probs=False):\n        \"\"\"\n        Run samples through tree until terminal nodes. Predict the Kaplan-Meier\n        estimator associated to the leaf node each sample ended into.\n\n        Args:\n            X (pd.DataFrame): Data frame with samples to generate predictions\n\n            return_ci (Bool): Whether to return confidence intervals via the Exponential Greenwood formula\n\n            return_interval_probs (Bool): Boolean indicating if interval probabilities are\n                supposed to be returned. If False the cumulative survival is returned.\n\n\n        Returns:\n            preds_df (pd.DataFrame): A dataframe of survival probabilities\n                for all times (columns), from a time_bins array, for all samples of X\n                (rows). If return_interval_probs is True, the interval probabilities are returned\n                instead of the cumulative survival probabilities.\n\n            upper_ci (np.array): Upper confidence interval for the survival\n                probability values\n\n            lower_ci (np.array): Lower confidence interval for the survival\n                probability values\n        \"\"\"\n        # getting leaves and extracting neighbors\n        leaves = self.feature_extractor.predict_leaves(X)\n\n        # searching for kaplan meier curves in leaves\n        preds_df = self._train_survival.loc[leaves].reset_index(drop=True)\n        upper_ci = self._train_upper_ci.loc[leaves].reset_index(drop=True)\n        lower_ci = self._train_lower_ci.loc[leaves].reset_index(drop=True)\n\n        if return_ci and return_interval_probs:\n            raise ValueError(\n                \"Confidence intervals for interval probabilities is not supported. Choose between return_ci and return_interval_probs.\"\n            )\n\n        if return_interval_probs:\n            preds_df = calculate_interval_failures(preds_df)\n            return preds_df\n\n        if return_ci:\n            return preds_df, upper_ci, lower_ci\n        return preds_df\n</code></pre>"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.__init__","title":"<code>__init__(self, xgb_params=None, enable_categorical=False)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xgb_params</code> <code>Dict</code> <p>Parameters for XGBoost model. If not passed, the following default parameters will be used:</p> <pre><code>DEFAULT_PARAMS_TREE = {\n    \"objective\": \"survival:cox\",\n    \"eval_metric\": \"cox-nloglik\",\n    \"tree_method\": \"hist\",\n    \"max_depth\": 100,\n    \"booster\": \"dart\",\n    \"subsample\": 1.0,\n    \"min_child_weight\": 30,\n    \"colsample_bynode\": 1.0,\n}</code></pre> <p>Check https://xgboost.readthedocs.io/en/latest/parameter.html for more options.</p> <code>None</code> Source code in <code>xgbse/_kaplan_neighbors.py</code> <pre><code>def __init__(\n    self,\n    xgb_params: Optional[Dict[str, Any]] = None,\n    enable_categorical: bool = False,\n):\n    \"\"\"\n    Args:\n        xgb_params (Dict): Parameters for XGBoost model.\n            If not passed, the following default parameters will be used:\n\n            ```\n            DEFAULT_PARAMS_TREE = {\n                \"objective\": \"survival:cox\",\n                \"eval_metric\": \"cox-nloglik\",\n                \"tree_method\": \"hist\",\n                \"max_depth\": 100,\n                \"booster\": \"dart\",\n                \"subsample\": 1.0,\n                \"min_child_weight\": 30,\n                \"colsample_bynode\": 1.0,\n            }\n            ```\n\n            Check &lt;https://xgboost.readthedocs.io/en/latest/parameter.html&gt; for more options.\n    \"\"\"\n    if xgb_params is None:\n        xgb_params = DEFAULT_PARAMS_TREE\n\n    super().__init__(xgb_params=xgb_params, enable_categorical=enable_categorical)\n    self.index_id = None\n</code></pre>"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.fit","title":"<code>fit(self, X, y, persist_train=True, index_id=None, time_bins=None, ci_width=0.683)</code>","text":"<p>Fit a single decision tree using xgboost. For each leaf in the tree, build a Kaplan-Meier estimator.</p> <p>Note</p> <ul> <li>Differently from <code>XGBSEKaplanNeighbors</code>, in <code>XGBSEKaplanTree</code>, the width of the confidence interval (<code>ci_width</code>) must be specified at fit time.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>[pd.DataFrame, np.array]</code> <p>Design matrix to fit XGBoost model</p> required <code>y</code> <code>structured array(numpy.bool_, numpy.number</code> <p>Binary event indicator as first field, and time of event or time of censoring as second field.</p> required <code>persist_train</code> <code>Bool</code> <p>Whether or not to persist training data to use explainability through prototypes</p> <code>True</code> <code>index_id</code> <code>pd.Index</code> <p>User defined index if intended to use explainability through prototypes</p> <code>None</code> <code>time_bins</code> <code>np.array</code> <p>Specified time windows to use when making survival predictions</p> <code>None</code> <code>ci_width</code> <code>Float</code> <p>Width of confidence interval</p> <code>0.683</code> <p>Returns:</p> Type Description <code>XGBSEKaplanTree</code> <p>Trained instance of XGBSEKaplanTree</p> Source code in <code>xgbse/_kaplan_neighbors.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    persist_train: bool = True,\n    index_id=None,\n    time_bins: Optional[Sequence] = None,\n    ci_width: float = 0.683,\n):\n    \"\"\"\n    Fit a single decision tree using xgboost. For each leaf in the tree,\n    build a Kaplan-Meier estimator.\n\n    !!! Note\n        * Differently from `XGBSEKaplanNeighbors`, in `XGBSEKaplanTree`,\n        the width of the confidence interval (`ci_width`)\n        must be specified at fit time.\n\n    Args:\n\n        X ([pd.DataFrame, np.array]): Design matrix to fit XGBoost model\n\n        y (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n            and time of event or time of censoring as second field.\n\n        persist_train (Bool): Whether or not to persist training data to use explainability\n            through prototypes\n\n        index_id (pd.Index): User defined index if intended to use explainability\n            through prototypes\n\n        time_bins (np.array): Specified time windows to use when making survival predictions\n\n        ci_width (Float): Width of confidence interval\n\n    Returns:\n        XGBSEKaplanTree: Trained instance of XGBSEKaplanTree\n    \"\"\"\n\n    self.feature_extractor.fit(\n        X,\n        y,\n        time_bins=time_bins,\n        num_boost_round=1,\n    )\n    self.feature_importances_ = self.feature_extractor.feature_importances_\n\n    E_train, T_train = convert_y(y)\n\n    self.time_bins = self.feature_extractor.time_bins\n    # getting leaves\n    leaves = self.feature_extractor.predict_leaves(X)\n\n    # organizing elements per leaf\n    leaf_neighs = (\n        pd.DataFrame({\"leaf\": leaves})\n        .groupby(\"leaf\")\n        .apply(lambda x: list(x.index))\n    )\n\n    # getting T and E for each leaf\n    T_leaves = _align_leaf_target(leaf_neighs, T_train)\n    E_leaves = _align_leaf_target(leaf_neighs, E_train)\n\n    # calculating z-score from width\n    z = st.norm.ppf(0.5 + ci_width / 2)\n\n    # vectorized (very fast!) implementation of Kaplan Meier curves\n    (\n        self._train_survival,\n        self._train_upper_ci,\n        self._train_lower_ci,\n    ) = calculate_kaplan_vectorized(T_leaves, E_leaves, self.time_bins, z)\n\n    # adding leaf indexes\n    self._train_survival = self._train_survival.set_index(leaf_neighs.index)\n    self._train_upper_ci = self._train_upper_ci.set_index(leaf_neighs.index)\n    self._train_lower_ci = self._train_lower_ci.set_index(leaf_neighs.index)\n\n    if persist_train:\n        self.persist_train = True\n        if index_id is None:\n            index_id = X.index.copy()\n        self.tree = BallTree(leaves.reshape(-1, 1), metric=\"hamming\", leaf_size=40)\n    self.index_id = index_id\n\n    return self\n</code></pre>"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.predict","title":"<code>predict(self, X, return_ci=False, return_interval_probs=False)</code>","text":"<p>Run samples through tree until terminal nodes. Predict the Kaplan-Meier estimator associated to the leaf node each sample ended into.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pd.DataFrame</code> <p>Data frame with samples to generate predictions</p> required <code>return_ci</code> <code>Bool</code> <p>Whether to return confidence intervals via the Exponential Greenwood formula</p> <code>False</code> <code>return_interval_probs</code> <code>Bool</code> <p>Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>preds_df (pd.DataFrame)</code> <p>A dataframe of survival probabilities     for all times (columns), from a time_bins array, for all samples of X     (rows). If return_interval_probs is True, the interval probabilities are returned     instead of the cumulative survival probabilities.</p> <p>upper_ci (np.array): Upper confidence interval for the survival     probability values</p> <p>lower_ci (np.array): Lower confidence interval for the survival     probability values</p> Source code in <code>xgbse/_kaplan_neighbors.py</code> <pre><code>def predict(self, X, return_ci=False, return_interval_probs=False):\n    \"\"\"\n    Run samples through tree until terminal nodes. Predict the Kaplan-Meier\n    estimator associated to the leaf node each sample ended into.\n\n    Args:\n        X (pd.DataFrame): Data frame with samples to generate predictions\n\n        return_ci (Bool): Whether to return confidence intervals via the Exponential Greenwood formula\n\n        return_interval_probs (Bool): Boolean indicating if interval probabilities are\n            supposed to be returned. If False the cumulative survival is returned.\n\n\n    Returns:\n        preds_df (pd.DataFrame): A dataframe of survival probabilities\n            for all times (columns), from a time_bins array, for all samples of X\n            (rows). If return_interval_probs is True, the interval probabilities are returned\n            instead of the cumulative survival probabilities.\n\n        upper_ci (np.array): Upper confidence interval for the survival\n            probability values\n\n        lower_ci (np.array): Lower confidence interval for the survival\n            probability values\n    \"\"\"\n    # getting leaves and extracting neighbors\n    leaves = self.feature_extractor.predict_leaves(X)\n\n    # searching for kaplan meier curves in leaves\n    preds_df = self._train_survival.loc[leaves].reset_index(drop=True)\n    upper_ci = self._train_upper_ci.loc[leaves].reset_index(drop=True)\n    lower_ci = self._train_lower_ci.loc[leaves].reset_index(drop=True)\n\n    if return_ci and return_interval_probs:\n        raise ValueError(\n            \"Confidence intervals for interval probabilities is not supported. Choose between return_ci and return_interval_probs.\"\n        )\n\n    if return_interval_probs:\n        preds_df = calculate_interval_failures(preds_df)\n        return preds_df\n\n    if return_ci:\n        return preds_df, upper_ci, lower_ci\n    return preds_df\n</code></pre>"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.set_fit_request","title":"<code>set_fit_request(self, *, ci_width='$UNCHANGED$', index_id='$UNCHANGED$', persist_train='$UNCHANGED$', time_bins='$UNCHANGED$')</code>","text":"<p>Request metadata passed to the <code>fit</code> method.</p> <p>Note that this method is only relevant if <code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>). Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing mechanism works.</p> <p>The options for each parameter are:</p> <ul> <li> <p><code>True</code>: metadata is requested, and passed to <code>fit</code> if provided. The request is ignored if metadata is not provided.</p> </li> <li> <p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>fit</code>.</p> </li> <li> <p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p> </li> <li> <p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p> </li> </ul> <p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the existing request. This allows you to change the request for some parameters and not others.</p> <p>.. versionadded:: 1.3</p> <p>.. note::     This method is only relevant if this estimator is used as a     sub-estimator of a meta-estimator, e.g. used inside a     :class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.set_fit_request--parameters","title":"Parameters","text":"<p>ci_width : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>ci_width</code> parameter in <code>fit</code>.</p> <p>index_id : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>index_id</code> parameter in <code>fit</code>.</p> <p>persist_train : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>persist_train</code> parameter in <code>fit</code>.</p> <p>time_bins : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>time_bins</code> parameter in <code>fit</code>.</p>"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.set_fit_request--returns","title":"Returns","text":"<p>self : object     The updated object.</p> Source code in <code>xgbse/_kaplan_neighbors.py</code> <pre><code>def func(*args, **kw):\n    \"\"\"Updates the request for provided parameters\n\n    This docstring is overwritten below.\n    See REQUESTER_DOC for expected functionality\n    \"\"\"\n    if not _routing_enabled():\n        raise RuntimeError(\n            \"This method is only available when metadata routing is enabled.\"\n            \" You can enable it using\"\n            \" sklearn.set_config(enable_metadata_routing=True).\"\n        )\n\n    if self.validate_keys and (set(kw) - set(self.keys)):\n        raise TypeError(\n            f\"Unexpected args: {set(kw) - set(self.keys)} in {self.name}. \"\n            f\"Accepted arguments are: {set(self.keys)}\"\n        )\n\n    # This makes it possible to use the decorated method as an unbound method,\n    # for instance when monkeypatching.\n    # https://github.com/scikit-learn/scikit-learn/issues/28632\n    if instance is None:\n        _instance = args[0]\n        args = args[1:]\n    else:\n        _instance = instance\n\n    # Replicating python's behavior when positional args are given other than\n    # `self`, and `self` is only allowed if this method is unbound.\n    if args:\n        raise TypeError(\n            f\"set_{self.name}_request() takes 0 positional argument but\"\n            f\" {len(args)} were given\"\n        )\n\n    requests = _instance._get_metadata_request()\n    method_metadata_request = getattr(requests, self.name)\n\n    for prop, alias in kw.items():\n        if alias is not UNCHANGED:\n            method_metadata_request.add_request(param=prop, alias=alias)\n    _instance._metadata_request = requests\n\n    return _instance\n</code></pre>"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.set_predict_request","title":"<code>set_predict_request(self, *, return_ci='$UNCHANGED$', return_interval_probs='$UNCHANGED$')</code>","text":"<p>Request metadata passed to the <code>predict</code> method.</p> <p>Note that this method is only relevant if <code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>). Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing mechanism works.</p> <p>The options for each parameter are:</p> <ul> <li> <p><code>True</code>: metadata is requested, and passed to <code>predict</code> if provided. The request is ignored if metadata is not provided.</p> </li> <li> <p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>predict</code>.</p> </li> <li> <p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p> </li> <li> <p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p> </li> </ul> <p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the existing request. This allows you to change the request for some parameters and not others.</p> <p>.. versionadded:: 1.3</p> <p>.. note::     This method is only relevant if this estimator is used as a     sub-estimator of a meta-estimator, e.g. used inside a     :class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.set_predict_request--parameters","title":"Parameters","text":"<p>return_ci : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>return_ci</code> parameter in <code>predict</code>.</p> <p>return_interval_probs : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>return_interval_probs</code> parameter in <code>predict</code>.</p>"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.set_predict_request--returns","title":"Returns","text":"<p>self : object     The updated object.</p> Source code in <code>xgbse/_kaplan_neighbors.py</code> <pre><code>def func(*args, **kw):\n    \"\"\"Updates the request for provided parameters\n\n    This docstring is overwritten below.\n    See REQUESTER_DOC for expected functionality\n    \"\"\"\n    if not _routing_enabled():\n        raise RuntimeError(\n            \"This method is only available when metadata routing is enabled.\"\n            \" You can enable it using\"\n            \" sklearn.set_config(enable_metadata_routing=True).\"\n        )\n\n    if self.validate_keys and (set(kw) - set(self.keys)):\n        raise TypeError(\n            f\"Unexpected args: {set(kw) - set(self.keys)} in {self.name}. \"\n            f\"Accepted arguments are: {set(self.keys)}\"\n        )\n\n    # This makes it possible to use the decorated method as an unbound method,\n    # for instance when monkeypatching.\n    # https://github.com/scikit-learn/scikit-learn/issues/28632\n    if instance is None:\n        _instance = args[0]\n        args = args[1:]\n    else:\n        _instance = instance\n\n    # Replicating python's behavior when positional args are given other than\n    # `self`, and `self` is only allowed if this method is unbound.\n    if args:\n        raise TypeError(\n            f\"set_{self.name}_request() takes 0 positional argument but\"\n            f\" {len(args)} were given\"\n        )\n\n    requests = _instance._get_metadata_request()\n    method_metadata_request = getattr(requests, self.name)\n\n    for prop, alias in kw.items():\n        if alias is not UNCHANGED:\n            method_metadata_request.add_request(param=prop, alias=alias)\n    _instance._metadata_request = requests\n\n    return _instance\n</code></pre>"},{"location":"modules/meta.html","title":"xgbse._meta.XGBSEBootstrapEstimator","text":"<p>Bootstrap meta-estimator for XGBSE models:</p> <ul> <li>allows for confidence interval estimation for <code>XGBSEDebiasedBCE</code> and <code>XGBSEStackedWeibull</code></li> <li>provides variance stabilization for all models, specially for <code>XGBSEKaplanTree</code></li> </ul> <p>Performs simple bootstrap with sample size equal to training set size.</p> Source code in <code>xgbse/_meta.py</code> <pre><code>class XGBSEBootstrapEstimator(BaseEstimator):\n    \"\"\"\n    Bootstrap meta-estimator for XGBSE models:\n\n    *  allows for confidence interval estimation for `XGBSEDebiasedBCE` and `XGBSEStackedWeibull`\n    *  provides variance stabilization for all models, specially for `XGBSEKaplanTree`\n\n    Performs simple bootstrap with sample size equal to training set size.\n\n    \"\"\"\n\n    def __init__(self, base_estimator, n_estimators=10, random_state=42):\n        \"\"\"\n        Args:\n            base_estimator (XGBSEBaseEstimator): Base estimator for bootstrap procedure\n            n_estimators (int): Number of estimators to fit in bootstrap procedure\n            random_state (int): Random state for resampling function\n        \"\"\"\n        self.base_estimator = base_estimator\n        self.n_estimators = n_estimators\n        self.random_state = random_state\n\n    def fit(self, X, y, **kwargs):\n        \"\"\"\n        Fit several (base) estimators and store them.\n\n        Args:\n            X ([pd.DataFrame, np.array]): Features to be used while fitting\n                XGBoost model\n\n            y (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n                and time of event or time of censoring as second field.\n\n            **kwargs : Keyword arguments to be passed to .fit() method of base_estimator\n\n        Returns:\n            XGBSEBootstrapEstimator: Trained instance of XGBSEBootstrapEstimator\n\n        \"\"\"\n\n        # initializing list of estimators\n        self.estimators_ = []\n\n        # loop for n_estimators\n        for i in range(self.n_estimators):\n            X_sample, y_sample = resample(X, y, random_state=i + self.random_state)\n\n            trained_model = self.base_estimator.fit(X_sample, y_sample, **kwargs)\n\n            self.estimators_.append(deepcopy(trained_model))\n\n        return self\n\n    def predict(self, X, return_ci=False, ci_width=0.683, return_interval_probs=False):\n        \"\"\"\n        Predicts survival as given by the base estimator. A survival function, its upper and lower\n        confidence intervals can be returned for each sample of the dataframe X.\n\n        Args:\n            X (pd.DataFrame): data frame with samples to generate predictions\n\n            return_ci (Bool): whether to include confidence intervals\n\n            ci_width (Float): width of confidence interval\n\n        Returns:\n            ([(pd.DataFrame, np.array, np.array), pd.DataFrame]):\n            preds_df: A dataframe of survival probabilities\n            for all times (columns), from a time_bins array, for all samples of X\n            (rows). If return_interval_probs is True, the interval probabilities are returned\n            instead of the cumulative survival probabilities.\n\n            upper_ci: Upper confidence interval for the survival\n                probability values\n\n            lower_ci: Lower confidence interval for the survival\n                probability values\n        \"\"\"\n\n        preds_list = []\n\n        for estimator in self.estimators_:\n            temp_preds = estimator.predict(\n                X, return_interval_probs=return_interval_probs\n            )\n            preds_list.append(temp_preds)\n\n        agg_preds = pd.concat(preds_list)\n\n        preds_df = agg_preds.groupby(level=0).mean()\n\n        if return_ci:\n            low_p = 0.5 - ci_width / 2\n            high_p = 0.5 + ci_width / 2\n\n            lower_ci = agg_preds.groupby(level=0).quantile(low_p)\n            upper_ci = agg_preds.groupby(level=0).quantile(high_p)\n\n            return preds_df, upper_ci, lower_ci\n\n        return preds_df\n</code></pre>"},{"location":"modules/meta.html#xgbse._meta.XGBSEBootstrapEstimator.__init__","title":"<code>__init__(self, base_estimator, n_estimators=10, random_state=42)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>base_estimator</code> <code>XGBSEBaseEstimator</code> <p>Base estimator for bootstrap procedure</p> required <code>n_estimators</code> <code>int</code> <p>Number of estimators to fit in bootstrap procedure</p> <code>10</code> <code>random_state</code> <code>int</code> <p>Random state for resampling function</p> <code>42</code> Source code in <code>xgbse/_meta.py</code> <pre><code>def __init__(self, base_estimator, n_estimators=10, random_state=42):\n    \"\"\"\n    Args:\n        base_estimator (XGBSEBaseEstimator): Base estimator for bootstrap procedure\n        n_estimators (int): Number of estimators to fit in bootstrap procedure\n        random_state (int): Random state for resampling function\n    \"\"\"\n    self.base_estimator = base_estimator\n    self.n_estimators = n_estimators\n    self.random_state = random_state\n</code></pre>"},{"location":"modules/meta.html#xgbse._meta.XGBSEBootstrapEstimator.fit","title":"<code>fit(self, X, y, **kwargs)</code>","text":"<p>Fit several (base) estimators and store them.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>[pd.DataFrame, np.array]</code> <p>Features to be used while fitting XGBoost model</p> required <code>y</code> <code>structured array(numpy.bool_, numpy.number</code> <p>Binary event indicator as first field, and time of event or time of censoring as second field.</p> required <code>**kwargs</code> <p>Keyword arguments to be passed to .fit() method of base_estimator</p> <code>{}</code> <p>Returns:</p> Type Description <code>XGBSEBootstrapEstimator</code> <p>Trained instance of XGBSEBootstrapEstimator</p> Source code in <code>xgbse/_meta.py</code> <pre><code>def fit(self, X, y, **kwargs):\n    \"\"\"\n    Fit several (base) estimators and store them.\n\n    Args:\n        X ([pd.DataFrame, np.array]): Features to be used while fitting\n            XGBoost model\n\n        y (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n            and time of event or time of censoring as second field.\n\n        **kwargs : Keyword arguments to be passed to .fit() method of base_estimator\n\n    Returns:\n        XGBSEBootstrapEstimator: Trained instance of XGBSEBootstrapEstimator\n\n    \"\"\"\n\n    # initializing list of estimators\n    self.estimators_ = []\n\n    # loop for n_estimators\n    for i in range(self.n_estimators):\n        X_sample, y_sample = resample(X, y, random_state=i + self.random_state)\n\n        trained_model = self.base_estimator.fit(X_sample, y_sample, **kwargs)\n\n        self.estimators_.append(deepcopy(trained_model))\n\n    return self\n</code></pre>"},{"location":"modules/meta.html#xgbse._meta.XGBSEBootstrapEstimator.predict","title":"<code>predict(self, X, return_ci=False, ci_width=0.683, return_interval_probs=False)</code>","text":"<p>Predicts survival as given by the base estimator. A survival function, its upper and lower confidence intervals can be returned for each sample of the dataframe X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pd.DataFrame</code> <p>data frame with samples to generate predictions</p> required <code>return_ci</code> <code>Bool</code> <p>whether to include confidence intervals</p> <code>False</code> <code>ci_width</code> <code>Float</code> <p>width of confidence interval</p> <code>0.683</code> <p>Returns:</p> Type Description <code>([(pd.DataFrame, np.array, np.array), pd.DataFrame])</code> <p>preds_df: A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities.</p> <p>upper_ci: Upper confidence interval for the survival     probability values</p> <p>lower_ci: Lower confidence interval for the survival     probability values</p> Source code in <code>xgbse/_meta.py</code> <pre><code>def predict(self, X, return_ci=False, ci_width=0.683, return_interval_probs=False):\n    \"\"\"\n    Predicts survival as given by the base estimator. A survival function, its upper and lower\n    confidence intervals can be returned for each sample of the dataframe X.\n\n    Args:\n        X (pd.DataFrame): data frame with samples to generate predictions\n\n        return_ci (Bool): whether to include confidence intervals\n\n        ci_width (Float): width of confidence interval\n\n    Returns:\n        ([(pd.DataFrame, np.array, np.array), pd.DataFrame]):\n        preds_df: A dataframe of survival probabilities\n        for all times (columns), from a time_bins array, for all samples of X\n        (rows). If return_interval_probs is True, the interval probabilities are returned\n        instead of the cumulative survival probabilities.\n\n        upper_ci: Upper confidence interval for the survival\n            probability values\n\n        lower_ci: Lower confidence interval for the survival\n            probability values\n    \"\"\"\n\n    preds_list = []\n\n    for estimator in self.estimators_:\n        temp_preds = estimator.predict(\n            X, return_interval_probs=return_interval_probs\n        )\n        preds_list.append(temp_preds)\n\n    agg_preds = pd.concat(preds_list)\n\n    preds_df = agg_preds.groupby(level=0).mean()\n\n    if return_ci:\n        low_p = 0.5 - ci_width / 2\n        high_p = 0.5 + ci_width / 2\n\n        lower_ci = agg_preds.groupby(level=0).quantile(low_p)\n        upper_ci = agg_preds.groupby(level=0).quantile(high_p)\n\n        return preds_df, upper_ci, lower_ci\n\n    return preds_df\n</code></pre>"},{"location":"modules/meta.html#xgbse._meta.XGBSEBootstrapEstimator.set_predict_request","title":"<code>set_predict_request(self, *, ci_width='$UNCHANGED$', return_ci='$UNCHANGED$', return_interval_probs='$UNCHANGED$')</code>","text":"<p>Request metadata passed to the <code>predict</code> method.</p> <p>Note that this method is only relevant if <code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>). Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing mechanism works.</p> <p>The options for each parameter are:</p> <ul> <li> <p><code>True</code>: metadata is requested, and passed to <code>predict</code> if provided. The request is ignored if metadata is not provided.</p> </li> <li> <p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>predict</code>.</p> </li> <li> <p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p> </li> <li> <p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p> </li> </ul> <p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the existing request. This allows you to change the request for some parameters and not others.</p> <p>.. versionadded:: 1.3</p> <p>.. note::     This method is only relevant if this estimator is used as a     sub-estimator of a meta-estimator, e.g. used inside a     :class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>"},{"location":"modules/meta.html#xgbse._meta.XGBSEBootstrapEstimator.set_predict_request--parameters","title":"Parameters","text":"<p>ci_width : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>ci_width</code> parameter in <code>predict</code>.</p> <p>return_ci : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>return_ci</code> parameter in <code>predict</code>.</p> <p>return_interval_probs : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>return_interval_probs</code> parameter in <code>predict</code>.</p>"},{"location":"modules/meta.html#xgbse._meta.XGBSEBootstrapEstimator.set_predict_request--returns","title":"Returns","text":"<p>self : object     The updated object.</p> Source code in <code>xgbse/_meta.py</code> <pre><code>def func(*args, **kw):\n    \"\"\"Updates the request for provided parameters\n\n    This docstring is overwritten below.\n    See REQUESTER_DOC for expected functionality\n    \"\"\"\n    if not _routing_enabled():\n        raise RuntimeError(\n            \"This method is only available when metadata routing is enabled.\"\n            \" You can enable it using\"\n            \" sklearn.set_config(enable_metadata_routing=True).\"\n        )\n\n    if self.validate_keys and (set(kw) - set(self.keys)):\n        raise TypeError(\n            f\"Unexpected args: {set(kw) - set(self.keys)} in {self.name}. \"\n            f\"Accepted arguments are: {set(self.keys)}\"\n        )\n\n    # This makes it possible to use the decorated method as an unbound method,\n    # for instance when monkeypatching.\n    # https://github.com/scikit-learn/scikit-learn/issues/28632\n    if instance is None:\n        _instance = args[0]\n        args = args[1:]\n    else:\n        _instance = instance\n\n    # Replicating python's behavior when positional args are given other than\n    # `self`, and `self` is only allowed if this method is unbound.\n    if args:\n        raise TypeError(\n            f\"set_{self.name}_request() takes 0 positional argument but\"\n            f\" {len(args)} were given\"\n        )\n\n    requests = _instance._get_metadata_request()\n    method_metadata_request = getattr(requests, self.name)\n\n    for prop, alias in kw.items():\n        if alias is not UNCHANGED:\n            method_metadata_request.add_request(param=prop, alias=alias)\n    _instance._metadata_request = requests\n\n    return _instance\n</code></pre>"},{"location":"modules/metrics.html","title":"xgbse.metrics","text":""},{"location":"modules/metrics.html#xgbse.metrics.approx_brier_score","title":"<code>approx_brier_score(y_true, survival, aggregate='mean')</code>","text":"<p>Estimate brier score for all survival time windows. Aggregate scores for an approximate integrated brier score estimate.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>structured array(numpy.bool_, numpy.number</code> <p>B inary event indicator as first field, and time of event or time of censoring as second field.</p> required <code>survival</code> <code>[pd.DataFrame, np.array]</code> <p>A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample.</p> required <code>aggregate</code> <code>[string, None]</code> <p>How to aggregate brier scores from different time windows:</p> <ul> <li> <p><code>mean</code> takes simple average</p> </li> <li> <p><code>None</code> returns full list of brier scores for each time window</p> </li> </ul> <code>'mean'</code> <p>Returns:</p> Type Description <code>[Float, np.array]</code> <p>single value if aggregate is 'mean'     np.array if aggregate is None</p> Source code in <code>xgbse/metrics.py</code> <pre><code>def approx_brier_score(y_true, survival, aggregate=\"mean\"):\n    \"\"\"\n    Estimate brier score for all survival time windows. Aggregate scores for an approximate\n    integrated brier score estimate.\n\n    Args:\n        y_true (structured array(numpy.bool_, numpy.number)): B inary event indicator as first field,\n            and time of event or time of censoring as second field.\n\n        survival ([pd.DataFrame, np.array]): A dataframe of survival probabilities\n            for all times (columns), from a time_bins array, for all samples of X (rows).\n            If risk_strategy is 'precomputed', is an array with representing risks for each sample.\n\n        aggregate ([string, None]): How to aggregate brier scores from different time windows:\n\n            * `mean` takes simple average\n\n            * `None` returns full list of brier scores for each time window\n\n    Returns:\n        [Float, np.array]:\n            single value if aggregate is 'mean'\n            np.array if aggregate is None\n    \"\"\"\n    events, times = convert_y(y_true)\n    events = events.astype(bool)\n\n    # calculating censoring distribution\n    censoring_dist, _, _ = calculate_kaplan_vectorized(\n        times.reshape(1, -1), ~events.reshape(1, -1), survival.columns\n    )\n\n    # initializing scoring df\n    scoring_df = pd.DataFrame({\"e\": events, \"t\": times}, index=survival.index)\n\n    # adding censoring distribution survival at event\n    event_time_windows = _match_times_to_windows(times, survival.columns)\n    scoring_df[\"cens_at_event\"] = censoring_dist[event_time_windows].iloc[0].values\n\n    # list of window results\n    window_results = []\n\n    # loop for all suvival time windows\n    for window in survival.columns:\n        # adding window info to scoring df\n        scoring_df = scoring_df.assign(surv_at_window=survival[window]).assign(\n            cens_at_window=censoring_dist[window].values[0]\n        )\n\n        # calculating censored brier score first term\n        # as by formula on B4.3 of https://arxiv.org/pdf/1811.11347.pdf\n        first_term = (\n            (scoring_df[\"t\"] &lt;= window).astype(int)\n            * (scoring_df[\"e\"])\n            * (scoring_df[\"surv_at_window\"]) ** 2\n            / (scoring_df[\"cens_at_event\"])\n        )\n\n        # calculating censored brier score second term\n        # as by formula on B4.3 of https://arxiv.org/pdf/1811.11347.pdf\n        second_term = (\n            (scoring_df[\"t\"] &gt; window).astype(int)\n            * (1 - scoring_df[\"surv_at_window\"]) ** 2\n            / (scoring_df[\"cens_at_window\"])\n        )\n\n        # adding and taking average\n        result = (first_term + second_term).sum() / scoring_df.shape[0]\n        window_results.append(result)\n\n    if aggregate == \"mean\":\n        return np.array(window_results).mean()\n    elif aggregate is None:\n        return np.array(window_results)\n    else:\n        raise ValueError(\n            f\"Chosen aggregating strategy of {aggregate} is not available.\"\n        )\n</code></pre>"},{"location":"modules/metrics.html#xgbse.metrics.concordance_index","title":"<code>concordance_index(y_true, survival, risk_strategy='mean', which_window=None)</code>","text":"<p>Compute the C-index for a structured array of ground truth times and events and a predicted survival curve using different strategies for estimating risk from it.</p> <p>Note</p> <ul> <li>Computation of the C-index is \\(\\mathcal{O}(n^2)\\).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>structured array(numpy.bool_, numpy.number</code> <p>Binary event indicator as first field, and time of event or time of censoring as second field.</p> required <code>survival</code> <code>[pd.DataFrame, np.array]</code> <p>A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample.</p> required <code>risk_strategy</code> <code>string</code> <p>Strategy to compute risks from the survival curve. For a given sample:</p> <ul> <li> <p><code>mean</code> averages probabilities across all times</p> </li> <li> <p><code>window</code>: lets user choose on of the time windows available (by which_window argument)     and uses probabilities of this specific window</p> </li> <li> <p><code>midpoint</code>: selects the most central window of index int(survival.columns.shape[0]/2)     and uses probabilities of this specific window</p> </li> <li> <p><code>precomputed</code>: assumes user has already calculated risk.     The survival argument is assumed to contain an array of risks instead</p> </li> </ul> <code>'mean'</code> <code>which_window</code> <code>object</code> <p>Which window to use when risk_strategy is 'window'. Should be one of the columns of the dataframe. Will raise ValueError if column is not present</p> <code>None</code> <p>Returns:</p> Type Description <code>Float</code> <p>Concordance index for y_true and survival</p> Source code in <code>xgbse/metrics.py</code> <pre><code>def concordance_index(y_true, survival, risk_strategy=\"mean\", which_window=None):\n    \"\"\"\n    Compute the C-index for a structured array of ground truth times and events\n    and a predicted survival curve using different strategies for estimating risk from it.\n\n    !!! Note\n        * Computation of the C-index is $\\\\mathcal{O}(n^2)$.\n\n    Args:\n        y_true (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n            and time of event or time of censoring as second field.\n\n        survival ([pd.DataFrame, np.array]): A dataframe of survival probabilities\n            for all times (columns), from a time_bins array, for all samples of X (rows).\n            If risk_strategy is 'precomputed', is an array with representing risks for each sample.\n\n        risk_strategy (string):\n            Strategy to compute risks from the survival curve. For a given sample:\n\n            * `mean` averages probabilities across all times\n\n            * `window`: lets user choose on of the time windows available (by which_window argument)\n                and uses probabilities of this specific window\n\n            * `midpoint`: selects the most central window of index int(survival.columns.shape[0]/2)\n                and uses probabilities of this specific window\n\n            * `precomputed`: assumes user has already calculated risk.\n                The survival argument is assumed to contain an array of risks instead\n\n        which_window (object): Which window to use when risk_strategy is 'window'. Should be one\n            of the columns of the dataframe. Will raise ValueError if column is not present\n\n    Returns:\n        Float: Concordance index for y_true and survival\n    \"\"\"\n\n    # choosing risk calculation strategy\n\n    if risk_strategy == \"mean\":\n        risks = 1 - survival.mean(axis=1)\n\n    elif risk_strategy == \"window\":\n        if which_window is None:\n            raise ValueError(\n                \"Need to set which window to use via the which_window parameter\"\n            )\n        risks = 1 - survival[which_window]\n\n    elif risk_strategy == \"midpoint\":\n        midpoint = int(survival.columns.shape[0] / 2)\n        midpoint_col = survival.columns[midpoint]\n        risks = 1 - survival[midpoint_col]\n\n    elif risk_strategy == \"precomputed\":\n        risks = survival\n\n    else:\n        raise ValueError(\n            f\"Chosen risk computing strategy of {risk_strategy} is not available.\"\n        )\n\n    # organizing event, time and risk data\n    events, times = convert_y(y_true)\n    events = events.astype(bool)\n\n    cind_df = pd.DataFrame({\"t\": times, \"e\": events, \"r\": risks})\n\n    count_pairs = 0\n    concordant_pairs = 0\n    tied_pairs = 0\n\n    # running loop for each uncensored sample,\n    # as by https://arxiv.org/pdf/1811.11347.pdf\n    for _, row in cind_df.query(\"e == True\").iterrows():\n        # getting all censored and uncensored samples\n        # after current row\n        samples_after_i = cind_df.query(f\"\"\"{row['t']} &lt; t\"\"\")\n\n        # counting total, concordant and tied pairs\n        count_pairs += samples_after_i.shape[0]\n        concordant_pairs += (samples_after_i[\"r\"] &lt; row[\"r\"]).sum()\n        tied_pairs += (samples_after_i[\"r\"] == row[\"r\"]).sum()\n\n    return (concordant_pairs + tied_pairs / 2) / count_pairs\n</code></pre>"},{"location":"modules/metrics.html#xgbse.metrics.dist_calibration_score","title":"<code>dist_calibration_score(y_true, survival, n_bins=10, returns='pval')</code>","text":"<p>Estimate D-Calibration for the survival predictions.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>structured array(numpy.bool_, numpy.number</code> <p>Binary event indicator as first field, and time of event or time of censoring as second field.</p> required <code>survival</code> <code>[pd.DataFrame, np.array]</code> <p>A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample.</p> required <code>n_bins</code> <code>Int</code> <p>Number of bins to equally divide the [0, 1] interval</p> <code>10</code> <code>returns</code> <code>string</code> <p>What information to return from the function:</p> <ul> <li> <p><code>statistic</code> returns the chi squared test statistic</p> </li> <li> <p><code>pval</code> returns the chi squared test p value</p> </li> <li> <p><code>max_deviation</code> returns the maximum percentage deviation from the expected value, calculated as <code>abs(expected_percentage - real_percentage)</code>, where <code>expected_percentage = 1.0/n_bins</code></p> </li> <li> <p><code>histogram</code> returns the full calibration histogram per bin</p> </li> <li> <p><code>all</code> returns all of the above in a dictionary</p> </li> </ul> <code>'pval'</code> <p>Returns:</p> Type Description <code>[Float, np.array, Dict]</code> <ul> <li>Single value if returns is in `['statistic','pval','max_deviation']``</li> <li>np.array if returns is 'histogram'</li> <li>dict if returns is 'all'</li> </ul> Source code in <code>xgbse/metrics.py</code> <pre><code>def dist_calibration_score(y_true, survival, n_bins=10, returns=\"pval\"):\n    \"\"\"\n    Estimate D-Calibration for the survival predictions.\n\n    Args:\n        y_true (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n            and time of event or time of censoring as second field.\n\n        survival ([pd.DataFrame, np.array]): A dataframe of survival probabilities\n            for all times (columns), from a time_bins array, for all samples of X (rows).\n            If risk_strategy is 'precomputed', is an array with representing risks for each sample.\n\n        n_bins (Int): Number of bins to equally divide the [0, 1] interval\n\n        returns (string):\n            What information to return from the function:\n\n            * `statistic` returns the chi squared test statistic\n\n            * `pval` returns the chi squared test p value\n\n            * `max_deviation` returns the maximum percentage deviation from the expected value,\n            calculated as `abs(expected_percentage - real_percentage)`,\n            where `expected_percentage = 1.0/n_bins`\n\n            * `histogram` returns the full calibration histogram per bin\n\n            * `all` returns all of the above in a dictionary\n\n    Returns:\n        [Float, np.array, Dict]:\n        * Single value if returns is in `['statistic','pval','max_deviation']``\n        * np.array if returns is 'histogram'\n        * dict if returns is 'all'\n    \"\"\"\n\n    # calculating bins\n    bins = np.round(np.linspace(0, 1, n_bins + 1), 2)\n\n    events, times = convert_y(y_true)\n    events = events.astype(bool)\n\n    # mapping event and censoring times to survival windows\n    event_time_windows = _match_times_to_windows(times, survival.columns)\n    survival_at_ti = np.array(\n        [survival.iloc[i][event_time_windows[i]] for i in range(len(survival))]\n    )\n    survival_at_ti = np.clip(survival_at_ti, EPS, None)\n\n    # creating data frame to calculate uncensored and censored counts\n    scoring_df = pd.DataFrame(\n        {\n            \"survival_at_ti\": survival_at_ti,\n            \"t\": times,\n            \"e\": events,\n            \"bin\": pd.cut(survival_at_ti, bins, include_lowest=True),\n            \"cens_spill_term\": 1 / (n_bins * survival_at_ti),\n        }\n    )\n\n    # computing uncensored counts:\n    # sum the number of events per bin\n    count_uncens = scoring_df.query(\"e == True\").groupby(\"bin\").size()\n\n    # computing censored counts at bin of censoring\n    # formula (A) as by page 49 of\n    # https://arxiv.org/pdf/1811.11347.pdf\n    count_cens = (\n        scoring_df.query(\"e == False\")\n        .groupby(\"bin\")\n        .apply(lambda x: (1 - np.clip(x.name.left, 0, 1) / x[\"survival_at_ti\"]).sum())\n    )\n\n    # computing censored counts at bins after censoring\n    # effect of 'blurring'\n    # formula (B) as by page 49 of\n    # https://arxiv.org/pdf/1811.11347.pdf\n    count_cens_spill = (\n        scoring_df.query(\"e == False\")\n        .groupby(\"bin\")[\"cens_spill_term\"]\n        .sum()\n        .iloc[::-1]\n        .shift()\n        .fillna(0)\n        .cumsum()\n        .iloc[::-1]\n    )\n\n    final_bin_counts = count_uncens + count_cens + count_cens_spill\n\n    if returns == \"statistic\":\n        result = chisquare(final_bin_counts)\n        return result.statistic\n\n    elif returns == \"pval\":\n        result = chisquare(final_bin_counts)\n        return result.pvalue\n\n    elif returns == \"max_deviation\":\n        proportions = final_bin_counts / final_bin_counts.sum()\n        return np.abs(proportions - 1 / n_bins).max()\n\n    elif returns == \"histogram\":\n        return final_bin_counts\n\n    elif returns == \"all\":\n        result = chisquare(final_bin_counts)\n        proportions = final_bin_counts / final_bin_counts.sum()\n        max_deviation = np.abs(proportions - 1 / n_bins).max()\n        return {\n            \"statistic\": result.statistic,\n            \"pval\": result.pvalue,\n            \"max_deviation\": max_deviation,\n            \"histogram\": final_bin_counts,\n        }\n    else:\n        raise ValueError(f\"Chosen return of {returns} is not available.\")\n</code></pre>"},{"location":"modules/stacked_weibull.html","title":"xgbse._stacked_weibull.XGBSEStackedWeibull","text":"<p>Perform stacking of a XGBoost survival model with a Weibull AFT parametric model. The XGBoost fits the data and then predicts a value that is interpreted as a risk metric. This risk metric is fed to the Weibull regression which uses it as its only independent variable.</p> <p>Thus, we can get the benefit of XGBoost discrimination power alongside the Weibull AFT statistical rigor (e.g. calibrated survival curves).</p> <p>Note</p> <ul> <li>As we're stacking XGBoost with a single, one-variable parametric model (as opposed to <code>XGBSEDebiasedBCE</code>), the model can be much faster (especially in training).</li> <li>We also have better extrapolation capabilities, as opposed to the cure fraction problem in <code>XGBSEKaplanNeighbors</code> and <code>XGBSEKaplanTree</code>.</li> <li>However, we also have stronger assumptions about the shape of the survival curve.</li> </ul> <p>Read more in How XGBSE works.</p> Source code in <code>xgbse/_stacked_weibull.py</code> <pre><code>class XGBSEStackedWeibull(XGBSEBaseEstimator):\n    \"\"\"\n    Perform stacking of a XGBoost survival model with a Weibull AFT parametric model.\n    The XGBoost fits the data and then predicts a value that is interpreted as a risk metric.\n    This risk metric is fed to the Weibull regression which uses it as its only independent variable.\n\n    Thus, we can get the benefit of XGBoost discrimination power alongside the Weibull AFT\n    statistical rigor (e.g. calibrated survival curves).\n\n    !!! Note\n        * As we're stacking XGBoost with a single, one-variable parametric model\n        (as opposed to `XGBSEDebiasedBCE`), the model can be much faster (especially in training).\n        * We also have better extrapolation capabilities, as opposed to the cure fraction\n        problem in `XGBSEKaplanNeighbors` and `XGBSEKaplanTree`.\n        * However, we also have stronger assumptions about the shape of the survival curve.\n\n    Read more in [How XGBSE works](https://loft-br.github.io/xgboost-survival-embeddings/how_xgbse_works.html).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        xgb_params: Optional[Dict[str, Any]] = None,\n        weibull_params: Optional[Dict[str, Any]] = {},\n        enable_categorical: bool = False,\n    ):\n        \"\"\"\n        Args:\n            xgb_params (Dict, None): Parameters for XGBoost model.\n                If None, will use XGBoost defaults and set objective as `survival:aft`.\n                Check &lt;https://xgboost.readthedocs.io/en/latest/parameter.html&gt; for options.\n\n            weibull_params (Dict): Parameters for Weibull Regerssion model.\n                If not passed, will use the default parameters as shown in the Lifelines documentation.\n                Check &lt;https://lifelines.readthedocs.io/en/latest/fitters/regression/WeibullAFTFitter.html&gt;\n                for more options.\n\n            enable_categorical (bool): Enable categorical feature support on xgboost model\n\n        \"\"\"\n        self.feature_extractor = FeatureExtractor(\n            xgb_params=xgb_params, enable_categorical=enable_categorical\n        )\n        self.xgb_params = self.feature_extractor.xgb_params\n        self.weibull_params = weibull_params\n\n        self.persist_train = False\n        self.feature_importances_ = None\n\n    def fit(\n        self,\n        X,\n        y,\n        time_bins: Optional[Sequence] = None,\n        validation_data: Optional[List[Tuple[Any, Any]]] = None,\n        num_boost_round: int = 10,\n        early_stopping_rounds: Optional[int] = None,\n        verbose_eval: int = 0,\n        persist_train: bool = False,\n        index_id=None,\n    ):\n        \"\"\"\n        Fit XGBoost model to predict a value that is interpreted as a risk metric.\n        Fit Weibull Regression model using risk metric as only independent variable.\n\n        Args:\n            X ([pd.DataFrame, np.array]): Features to be used while fitting XGBoost model\n\n            y (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n                and time of event or time of censoring as second field.\n\n            num_boost_round (Int): Number of boosting iterations.\n\n            validation_data (Tuple): Validation data in the format of a list of tuples [(X, y)]\n                if user desires to use early stopping\n\n            early_stopping_rounds (Int): Activates early stopping.\n                Validation metric needs to improve at least once\n                in every **early_stopping_rounds** round(s) to continue training.\n                See xgboost.train documentation.\n\n            verbose_eval ([Bool, Int]): Level of verbosity. See xgboost.train documentation.\n\n            persist_train (Bool): Whether or not to persist training data to use explainability\n                through prototypes\n\n            index_id (pd.Index): User defined index if intended to use explainability\n                through prototypes\n\n            time_bins (np.array): Specified time windows to use when making survival predictions\n\n        Returns:\n            XGBSEStackedWeibull: Trained XGBSEStackedWeibull instance\n        \"\"\"\n\n        self.fit_feature_extractor(\n            X,\n            y,\n            time_bins=time_bins,\n            validation_data=validation_data,\n            num_boost_round=num_boost_round,\n            early_stopping_rounds=early_stopping_rounds,\n            verbose_eval=verbose_eval,\n        )\n        E_train, T_train = convert_y(y)\n\n        # predicting hazard ratio from XGBoost\n        train_risk = self.feature_extractor.predict_hazard(X)\n\n        # replacing 0 by minimum positive value in df\n        # so Weibull can be fitted\n        min_positive_value = T_train[T_train &gt; 0].min()\n        T_train = np.clip(T_train, min_positive_value, None)\n\n        # creating df to use lifelines API\n        weibull_train_df = pd.DataFrame(\n            {\"risk\": train_risk, \"duration\": T_train, \"event\": E_train}\n        )\n\n        # fitting weibull aft\n        self.weibull_aft = WeibullAFTFitter(**self.weibull_params)\n        self.weibull_aft.fit(weibull_train_df, \"duration\", \"event\", ancillary=True)\n\n        if persist_train:\n            self.persist_train = True\n            if index_id is None:\n                index_id = X.index.copy()\n\n            index_leaves = self.feature_extractor.predict_leaves(X)\n            self.tree = BallTree(index_leaves, metric=\"hamming\")\n\n        self.index_id = index_id\n\n        return self\n\n    def predict(self, X, return_interval_probs=False):\n        \"\"\"\n        Predicts survival probabilities using the XGBoost + Weibull AFT stacking pipeline.\n\n        Args:\n            X (pd.DataFrame): Dataframe of features to be used as input for the\n                XGBoost model.\n\n            return_interval_probs (Bool): Boolean indicating if interval probabilities are\n                supposed to be returned. If False the cumulative survival is returned.\n                Default is False.\n\n        Returns:\n            pd.DataFrame: A dataframe of survival probabilities\n            for all times (columns), from a time_bins array, for all samples of X\n            (rows). If return_interval_probs is True, the interval probabilities are returned\n            instead of the cumulative survival probabilities.\n        \"\"\"\n        risk = self.feature_extractor.predict_hazard(X)\n        weibull_score_df = pd.DataFrame({\"risk\": risk})\n\n        preds_df = self.weibull_aft.predict_survival_function(\n            weibull_score_df, self.time_bins\n        ).T\n\n        if return_interval_probs:\n            preds_df = calculate_interval_failures(preds_df)\n\n        return preds_df\n</code></pre>"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull.__init__","title":"<code>__init__(self, xgb_params=None, weibull_params={}, enable_categorical=False)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xgb_params</code> <code>Dict, None</code> <p>Parameters for XGBoost model. If None, will use XGBoost defaults and set objective as <code>survival:aft</code>. Check https://xgboost.readthedocs.io/en/latest/parameter.html for options.</p> <code>None</code> <code>weibull_params</code> <code>Dict</code> <p>Parameters for Weibull Regerssion model. If not passed, will use the default parameters as shown in the Lifelines documentation. Check https://lifelines.readthedocs.io/en/latest/fitters/regression/WeibullAFTFitter.html for more options.</p> <code>{}</code> <code>enable_categorical</code> <code>bool</code> <p>Enable categorical feature support on xgboost model</p> <code>False</code> Source code in <code>xgbse/_stacked_weibull.py</code> <pre><code>def __init__(\n    self,\n    xgb_params: Optional[Dict[str, Any]] = None,\n    weibull_params: Optional[Dict[str, Any]] = {},\n    enable_categorical: bool = False,\n):\n    \"\"\"\n    Args:\n        xgb_params (Dict, None): Parameters for XGBoost model.\n            If None, will use XGBoost defaults and set objective as `survival:aft`.\n            Check &lt;https://xgboost.readthedocs.io/en/latest/parameter.html&gt; for options.\n\n        weibull_params (Dict): Parameters for Weibull Regerssion model.\n            If not passed, will use the default parameters as shown in the Lifelines documentation.\n            Check &lt;https://lifelines.readthedocs.io/en/latest/fitters/regression/WeibullAFTFitter.html&gt;\n            for more options.\n\n        enable_categorical (bool): Enable categorical feature support on xgboost model\n\n    \"\"\"\n    self.feature_extractor = FeatureExtractor(\n        xgb_params=xgb_params, enable_categorical=enable_categorical\n    )\n    self.xgb_params = self.feature_extractor.xgb_params\n    self.weibull_params = weibull_params\n\n    self.persist_train = False\n    self.feature_importances_ = None\n</code></pre>"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull.fit","title":"<code>fit(self, X, y, time_bins=None, validation_data=None, num_boost_round=10, early_stopping_rounds=None, verbose_eval=0, persist_train=False, index_id=None)</code>","text":"<p>Fit XGBoost model to predict a value that is interpreted as a risk metric. Fit Weibull Regression model using risk metric as only independent variable.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>[pd.DataFrame, np.array]</code> <p>Features to be used while fitting XGBoost model</p> required <code>y</code> <code>structured array(numpy.bool_, numpy.number</code> <p>Binary event indicator as first field, and time of event or time of censoring as second field.</p> required <code>num_boost_round</code> <code>Int</code> <p>Number of boosting iterations.</p> <code>10</code> <code>validation_data</code> <code>Tuple</code> <p>Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping</p> <code>None</code> <code>early_stopping_rounds</code> <code>Int</code> <p>Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation.</p> <code>None</code> <code>verbose_eval</code> <code>[Bool, Int]</code> <p>Level of verbosity. See xgboost.train documentation.</p> <code>0</code> <code>persist_train</code> <code>Bool</code> <p>Whether or not to persist training data to use explainability through prototypes</p> <code>False</code> <code>index_id</code> <code>pd.Index</code> <p>User defined index if intended to use explainability through prototypes</p> <code>None</code> <code>time_bins</code> <code>np.array</code> <p>Specified time windows to use when making survival predictions</p> <code>None</code> <p>Returns:</p> Type Description <code>XGBSEStackedWeibull</code> <p>Trained XGBSEStackedWeibull instance</p> Source code in <code>xgbse/_stacked_weibull.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    time_bins: Optional[Sequence] = None,\n    validation_data: Optional[List[Tuple[Any, Any]]] = None,\n    num_boost_round: int = 10,\n    early_stopping_rounds: Optional[int] = None,\n    verbose_eval: int = 0,\n    persist_train: bool = False,\n    index_id=None,\n):\n    \"\"\"\n    Fit XGBoost model to predict a value that is interpreted as a risk metric.\n    Fit Weibull Regression model using risk metric as only independent variable.\n\n    Args:\n        X ([pd.DataFrame, np.array]): Features to be used while fitting XGBoost model\n\n        y (structured array(numpy.bool_, numpy.number)): Binary event indicator as first field,\n            and time of event or time of censoring as second field.\n\n        num_boost_round (Int): Number of boosting iterations.\n\n        validation_data (Tuple): Validation data in the format of a list of tuples [(X, y)]\n            if user desires to use early stopping\n\n        early_stopping_rounds (Int): Activates early stopping.\n            Validation metric needs to improve at least once\n            in every **early_stopping_rounds** round(s) to continue training.\n            See xgboost.train documentation.\n\n        verbose_eval ([Bool, Int]): Level of verbosity. See xgboost.train documentation.\n\n        persist_train (Bool): Whether or not to persist training data to use explainability\n            through prototypes\n\n        index_id (pd.Index): User defined index if intended to use explainability\n            through prototypes\n\n        time_bins (np.array): Specified time windows to use when making survival predictions\n\n    Returns:\n        XGBSEStackedWeibull: Trained XGBSEStackedWeibull instance\n    \"\"\"\n\n    self.fit_feature_extractor(\n        X,\n        y,\n        time_bins=time_bins,\n        validation_data=validation_data,\n        num_boost_round=num_boost_round,\n        early_stopping_rounds=early_stopping_rounds,\n        verbose_eval=verbose_eval,\n    )\n    E_train, T_train = convert_y(y)\n\n    # predicting hazard ratio from XGBoost\n    train_risk = self.feature_extractor.predict_hazard(X)\n\n    # replacing 0 by minimum positive value in df\n    # so Weibull can be fitted\n    min_positive_value = T_train[T_train &gt; 0].min()\n    T_train = np.clip(T_train, min_positive_value, None)\n\n    # creating df to use lifelines API\n    weibull_train_df = pd.DataFrame(\n        {\"risk\": train_risk, \"duration\": T_train, \"event\": E_train}\n    )\n\n    # fitting weibull aft\n    self.weibull_aft = WeibullAFTFitter(**self.weibull_params)\n    self.weibull_aft.fit(weibull_train_df, \"duration\", \"event\", ancillary=True)\n\n    if persist_train:\n        self.persist_train = True\n        if index_id is None:\n            index_id = X.index.copy()\n\n        index_leaves = self.feature_extractor.predict_leaves(X)\n        self.tree = BallTree(index_leaves, metric=\"hamming\")\n\n    self.index_id = index_id\n\n    return self\n</code></pre>"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull.predict","title":"<code>predict(self, X, return_interval_probs=False)</code>","text":"<p>Predicts survival probabilities using the XGBoost + Weibull AFT stacking pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pd.DataFrame</code> <p>Dataframe of features to be used as input for the XGBoost model.</p> required <code>return_interval_probs</code> <code>Bool</code> <p>Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities.</p> Source code in <code>xgbse/_stacked_weibull.py</code> <pre><code>def predict(self, X, return_interval_probs=False):\n    \"\"\"\n    Predicts survival probabilities using the XGBoost + Weibull AFT stacking pipeline.\n\n    Args:\n        X (pd.DataFrame): Dataframe of features to be used as input for the\n            XGBoost model.\n\n        return_interval_probs (Bool): Boolean indicating if interval probabilities are\n            supposed to be returned. If False the cumulative survival is returned.\n            Default is False.\n\n    Returns:\n        pd.DataFrame: A dataframe of survival probabilities\n        for all times (columns), from a time_bins array, for all samples of X\n        (rows). If return_interval_probs is True, the interval probabilities are returned\n        instead of the cumulative survival probabilities.\n    \"\"\"\n    risk = self.feature_extractor.predict_hazard(X)\n    weibull_score_df = pd.DataFrame({\"risk\": risk})\n\n    preds_df = self.weibull_aft.predict_survival_function(\n        weibull_score_df, self.time_bins\n    ).T\n\n    if return_interval_probs:\n        preds_df = calculate_interval_failures(preds_df)\n\n    return preds_df\n</code></pre>"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull.set_fit_request","title":"<code>set_fit_request(self, *, early_stopping_rounds='$UNCHANGED$', index_id='$UNCHANGED$', num_boost_round='$UNCHANGED$', persist_train='$UNCHANGED$', time_bins='$UNCHANGED$', validation_data='$UNCHANGED$', verbose_eval='$UNCHANGED$')</code>","text":"<p>Request metadata passed to the <code>fit</code> method.</p> <p>Note that this method is only relevant if <code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>). Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing mechanism works.</p> <p>The options for each parameter are:</p> <ul> <li> <p><code>True</code>: metadata is requested, and passed to <code>fit</code> if provided. The request is ignored if metadata is not provided.</p> </li> <li> <p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>fit</code>.</p> </li> <li> <p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p> </li> <li> <p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p> </li> </ul> <p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the existing request. This allows you to change the request for some parameters and not others.</p> <p>.. versionadded:: 1.3</p> <p>.. note::     This method is only relevant if this estimator is used as a     sub-estimator of a meta-estimator, e.g. used inside a     :class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull.set_fit_request--parameters","title":"Parameters","text":"<p>early_stopping_rounds : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>early_stopping_rounds</code> parameter in <code>fit</code>.</p> <p>index_id : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>index_id</code> parameter in <code>fit</code>.</p> <p>num_boost_round : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>num_boost_round</code> parameter in <code>fit</code>.</p> <p>persist_train : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>persist_train</code> parameter in <code>fit</code>.</p> <p>time_bins : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>time_bins</code> parameter in <code>fit</code>.</p> <p>validation_data : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>validation_data</code> parameter in <code>fit</code>.</p> <p>verbose_eval : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>verbose_eval</code> parameter in <code>fit</code>.</p>"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull.set_fit_request--returns","title":"Returns","text":"<p>self : object     The updated object.</p> Source code in <code>xgbse/_stacked_weibull.py</code> <pre><code>def func(*args, **kw):\n    \"\"\"Updates the request for provided parameters\n\n    This docstring is overwritten below.\n    See REQUESTER_DOC for expected functionality\n    \"\"\"\n    if not _routing_enabled():\n        raise RuntimeError(\n            \"This method is only available when metadata routing is enabled.\"\n            \" You can enable it using\"\n            \" sklearn.set_config(enable_metadata_routing=True).\"\n        )\n\n    if self.validate_keys and (set(kw) - set(self.keys)):\n        raise TypeError(\n            f\"Unexpected args: {set(kw) - set(self.keys)} in {self.name}. \"\n            f\"Accepted arguments are: {set(self.keys)}\"\n        )\n\n    # This makes it possible to use the decorated method as an unbound method,\n    # for instance when monkeypatching.\n    # https://github.com/scikit-learn/scikit-learn/issues/28632\n    if instance is None:\n        _instance = args[0]\n        args = args[1:]\n    else:\n        _instance = instance\n\n    # Replicating python's behavior when positional args are given other than\n    # `self`, and `self` is only allowed if this method is unbound.\n    if args:\n        raise TypeError(\n            f\"set_{self.name}_request() takes 0 positional argument but\"\n            f\" {len(args)} were given\"\n        )\n\n    requests = _instance._get_metadata_request()\n    method_metadata_request = getattr(requests, self.name)\n\n    for prop, alias in kw.items():\n        if alias is not UNCHANGED:\n            method_metadata_request.add_request(param=prop, alias=alias)\n    _instance._metadata_request = requests\n\n    return _instance\n</code></pre>"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull.set_predict_request","title":"<code>set_predict_request(self, *, return_interval_probs='$UNCHANGED$')</code>","text":"<p>Request metadata passed to the <code>predict</code> method.</p> <p>Note that this method is only relevant if <code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>). Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing mechanism works.</p> <p>The options for each parameter are:</p> <ul> <li> <p><code>True</code>: metadata is requested, and passed to <code>predict</code> if provided. The request is ignored if metadata is not provided.</p> </li> <li> <p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>predict</code>.</p> </li> <li> <p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p> </li> <li> <p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p> </li> </ul> <p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the existing request. This allows you to change the request for some parameters and not others.</p> <p>.. versionadded:: 1.3</p> <p>.. note::     This method is only relevant if this estimator is used as a     sub-estimator of a meta-estimator, e.g. used inside a     :class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull.set_predict_request--parameters","title":"Parameters","text":"<p>return_interval_probs : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED     Metadata routing for <code>return_interval_probs</code> parameter in <code>predict</code>.</p>"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull.set_predict_request--returns","title":"Returns","text":"<p>self : object     The updated object.</p> Source code in <code>xgbse/_stacked_weibull.py</code> <pre><code>def func(*args, **kw):\n    \"\"\"Updates the request for provided parameters\n\n    This docstring is overwritten below.\n    See REQUESTER_DOC for expected functionality\n    \"\"\"\n    if not _routing_enabled():\n        raise RuntimeError(\n            \"This method is only available when metadata routing is enabled.\"\n            \" You can enable it using\"\n            \" sklearn.set_config(enable_metadata_routing=True).\"\n        )\n\n    if self.validate_keys and (set(kw) - set(self.keys)):\n        raise TypeError(\n            f\"Unexpected args: {set(kw) - set(self.keys)} in {self.name}. \"\n            f\"Accepted arguments are: {set(self.keys)}\"\n        )\n\n    # This makes it possible to use the decorated method as an unbound method,\n    # for instance when monkeypatching.\n    # https://github.com/scikit-learn/scikit-learn/issues/28632\n    if instance is None:\n        _instance = args[0]\n        args = args[1:]\n    else:\n        _instance = instance\n\n    # Replicating python's behavior when positional args are given other than\n    # `self`, and `self` is only allowed if this method is unbound.\n    if args:\n        raise TypeError(\n            f\"set_{self.name}_request() takes 0 positional argument but\"\n            f\" {len(args)} were given\"\n        )\n\n    requests = _instance._get_metadata_request()\n    method_metadata_request = getattr(requests, self.name)\n\n    for prop, alias in kw.items():\n        if alias is not UNCHANGED:\n            method_metadata_request.add_request(param=prop, alias=alias)\n    _instance._metadata_request = requests\n\n    return _instance\n</code></pre>"}]}